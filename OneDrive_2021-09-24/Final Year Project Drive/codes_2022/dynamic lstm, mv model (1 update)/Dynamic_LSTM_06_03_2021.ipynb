{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic LSTM 06-03-2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMicyxToqMya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c4a9f7-5122-422e-bec5-610f7b14a783"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZUpAr9hmscg"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np           \r\n",
        "import numpy\r\n",
        "from numpy import array\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WePgv9O6r0I_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bca6be-0d9c-4820-eeda-e1e2a9cb67f1"
      },
      "source": [
        "stonks=['ADANIPORTS',\t'ASIANPAINT',\t'AXISBANK'\t,'BAJAJ-AUTO',\t'BAJFINANCE',\t'BPCL'\t,'BHARTIARTL',\t'BRITANNIA'\t,'CIPLA'\t,'DIVISLAB',\t'DRREDDY',\t'GAIL',\t'GRASIM',\t'HCLTECH',\t'HDFCBANK',\t'HEROMOTOCO'\t,'HINDALCO',\t'HINDUNILVR',\t'HDFC',\t'ICICIBANK',\t'ITC'\t,'IOC'\t,'INDUSINDBK'\t,'INFY',\t'JSWSTEEL'\t,'KOTAKBANK',\t'LT',\t'M&M',\t'NTPC',\t'ONGC',\t'POWERGRID'\t,'RELIANCE',\t'SBIN'\t,'SUNPHARMA',\t'TCS',\t'TATAMOTORS',\t'TATASTEEL',\t'TECHM',\t'TITAN',\t'UPL',\t'ULTRACEMCO',\t'WIPRO']\r\n",
        "len(stonks)                                       #list of nifty50 stocks after lagard removal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSaVvpyV29Hw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "fbfca475-c7a6-454e-e92f-8e207e97032b"
      },
      "source": [
        "x=pd.read_csv('/content/drive/My Drive/fyp/Lagard-less till 2021.csv',parse_dates=['Date'],index_col='Date')\r\n",
        "x = x.loc[\"2016-01-01\" :]                         #Since 2016-01-01, 5y(1238rows till 2020-12-31), + year 2021's rows\r\n",
        "x.reset_index(drop=True, inplace=True)\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADANIPORTS</th>\n",
              "      <th>ASIANPAINT</th>\n",
              "      <th>AXISBANK</th>\n",
              "      <th>BAJAJ-AUTO</th>\n",
              "      <th>BAJFINANCE</th>\n",
              "      <th>BPCL</th>\n",
              "      <th>BHARTIARTL</th>\n",
              "      <th>BRITANNIA</th>\n",
              "      <th>CIPLA</th>\n",
              "      <th>DIVISLAB</th>\n",
              "      <th>DRREDDY</th>\n",
              "      <th>GAIL</th>\n",
              "      <th>GRASIM</th>\n",
              "      <th>HCLTECH</th>\n",
              "      <th>HDFCBANK</th>\n",
              "      <th>HEROMOTOCO</th>\n",
              "      <th>HINDALCO</th>\n",
              "      <th>HINDUNILVR</th>\n",
              "      <th>HDFC</th>\n",
              "      <th>ICICIBANK</th>\n",
              "      <th>ITC</th>\n",
              "      <th>IOC</th>\n",
              "      <th>INDUSINDBK</th>\n",
              "      <th>INFY</th>\n",
              "      <th>JSWSTEEL</th>\n",
              "      <th>KOTAKBANK</th>\n",
              "      <th>LT</th>\n",
              "      <th>M&amp;M</th>\n",
              "      <th>NTPC</th>\n",
              "      <th>ONGC</th>\n",
              "      <th>POWERGRID</th>\n",
              "      <th>RELIANCE</th>\n",
              "      <th>SBIN</th>\n",
              "      <th>SUNPHARMA</th>\n",
              "      <th>TCS</th>\n",
              "      <th>TATAMOTORS</th>\n",
              "      <th>TATASTEEL</th>\n",
              "      <th>TECHM</th>\n",
              "      <th>TITAN</th>\n",
              "      <th>UPL</th>\n",
              "      <th>ULTRACEMCO</th>\n",
              "      <th>WIPRO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>267.55</td>\n",
              "      <td>878.75</td>\n",
              "      <td>449.90</td>\n",
              "      <td>2518.00</td>\n",
              "      <td>6095.85</td>\n",
              "      <td>896.05</td>\n",
              "      <td>340.50</td>\n",
              "      <td>2986.40</td>\n",
              "      <td>655.35</td>\n",
              "      <td>1162.50</td>\n",
              "      <td>3108.60</td>\n",
              "      <td>369.75</td>\n",
              "      <td>3767.75</td>\n",
              "      <td>845.85</td>\n",
              "      <td>1088.75</td>\n",
              "      <td>2686.05</td>\n",
              "      <td>84.90</td>\n",
              "      <td>856.55</td>\n",
              "      <td>1258.45</td>\n",
              "      <td>263.00</td>\n",
              "      <td>327.50</td>\n",
              "      <td>433.25</td>\n",
              "      <td>963.85</td>\n",
              "      <td>1105.25</td>\n",
              "      <td>1027.00</td>\n",
              "      <td>727.25</td>\n",
              "      <td>1289.20</td>\n",
              "      <td>1265.35</td>\n",
              "      <td>144.50</td>\n",
              "      <td>242.50</td>\n",
              "      <td>141.00</td>\n",
              "      <td>1015.35</td>\n",
              "      <td>227.80</td>\n",
              "      <td>815.55</td>\n",
              "      <td>2416.40</td>\n",
              "      <td>401.90</td>\n",
              "      <td>257.40</td>\n",
              "      <td>520.05</td>\n",
              "      <td>352.05</td>\n",
              "      <td>440.35</td>\n",
              "      <td>2824.00</td>\n",
              "      <td>556.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>257.95</td>\n",
              "      <td>880.80</td>\n",
              "      <td>438.40</td>\n",
              "      <td>2483.35</td>\n",
              "      <td>6052.45</td>\n",
              "      <td>891.45</td>\n",
              "      <td>326.80</td>\n",
              "      <td>2972.60</td>\n",
              "      <td>645.65</td>\n",
              "      <td>1149.70</td>\n",
              "      <td>3056.00</td>\n",
              "      <td>359.80</td>\n",
              "      <td>3657.25</td>\n",
              "      <td>845.95</td>\n",
              "      <td>1070.50</td>\n",
              "      <td>2637.80</td>\n",
              "      <td>80.75</td>\n",
              "      <td>859.00</td>\n",
              "      <td>1216.70</td>\n",
              "      <td>255.55</td>\n",
              "      <td>325.10</td>\n",
              "      <td>426.45</td>\n",
              "      <td>934.20</td>\n",
              "      <td>1078.90</td>\n",
              "      <td>1044.70</td>\n",
              "      <td>705.30</td>\n",
              "      <td>1255.95</td>\n",
              "      <td>1242.50</td>\n",
              "      <td>143.30</td>\n",
              "      <td>238.05</td>\n",
              "      <td>140.95</td>\n",
              "      <td>995.30</td>\n",
              "      <td>220.70</td>\n",
              "      <td>799.10</td>\n",
              "      <td>2369.60</td>\n",
              "      <td>377.05</td>\n",
              "      <td>256.90</td>\n",
              "      <td>516.40</td>\n",
              "      <td>351.40</td>\n",
              "      <td>433.85</td>\n",
              "      <td>2749.15</td>\n",
              "      <td>557.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>255.70</td>\n",
              "      <td>900.65</td>\n",
              "      <td>436.45</td>\n",
              "      <td>2502.35</td>\n",
              "      <td>5997.85</td>\n",
              "      <td>902.65</td>\n",
              "      <td>323.45</td>\n",
              "      <td>2966.70</td>\n",
              "      <td>640.85</td>\n",
              "      <td>1147.50</td>\n",
              "      <td>3070.65</td>\n",
              "      <td>372.25</td>\n",
              "      <td>3659.10</td>\n",
              "      <td>842.80</td>\n",
              "      <td>1062.40</td>\n",
              "      <td>2616.85</td>\n",
              "      <td>82.60</td>\n",
              "      <td>847.95</td>\n",
              "      <td>1209.40</td>\n",
              "      <td>256.70</td>\n",
              "      <td>324.85</td>\n",
              "      <td>442.65</td>\n",
              "      <td>935.25</td>\n",
              "      <td>1074.05</td>\n",
              "      <td>1083.10</td>\n",
              "      <td>707.75</td>\n",
              "      <td>1256.90</td>\n",
              "      <td>1249.20</td>\n",
              "      <td>142.25</td>\n",
              "      <td>241.85</td>\n",
              "      <td>139.15</td>\n",
              "      <td>1005.15</td>\n",
              "      <td>217.75</td>\n",
              "      <td>800.50</td>\n",
              "      <td>2348.95</td>\n",
              "      <td>374.45</td>\n",
              "      <td>274.30</td>\n",
              "      <td>521.90</td>\n",
              "      <td>347.70</td>\n",
              "      <td>435.30</td>\n",
              "      <td>2733.05</td>\n",
              "      <td>556.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>251.80</td>\n",
              "      <td>886.40</td>\n",
              "      <td>430.70</td>\n",
              "      <td>2485.50</td>\n",
              "      <td>6107.25</td>\n",
              "      <td>912.30</td>\n",
              "      <td>322.20</td>\n",
              "      <td>3001.70</td>\n",
              "      <td>652.10</td>\n",
              "      <td>1137.20</td>\n",
              "      <td>3046.25</td>\n",
              "      <td>372.70</td>\n",
              "      <td>3652.40</td>\n",
              "      <td>841.40</td>\n",
              "      <td>1067.10</td>\n",
              "      <td>2578.20</td>\n",
              "      <td>80.55</td>\n",
              "      <td>843.05</td>\n",
              "      <td>1209.30</td>\n",
              "      <td>250.10</td>\n",
              "      <td>315.10</td>\n",
              "      <td>450.85</td>\n",
              "      <td>943.95</td>\n",
              "      <td>1069.35</td>\n",
              "      <td>1058.20</td>\n",
              "      <td>703.60</td>\n",
              "      <td>1236.85</td>\n",
              "      <td>1226.15</td>\n",
              "      <td>141.45</td>\n",
              "      <td>237.55</td>\n",
              "      <td>139.50</td>\n",
              "      <td>1032.20</td>\n",
              "      <td>216.85</td>\n",
              "      <td>791.95</td>\n",
              "      <td>2381.60</td>\n",
              "      <td>365.90</td>\n",
              "      <td>268.75</td>\n",
              "      <td>525.10</td>\n",
              "      <td>347.85</td>\n",
              "      <td>443.35</td>\n",
              "      <td>2735.30</td>\n",
              "      <td>555.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>245.00</td>\n",
              "      <td>872.35</td>\n",
              "      <td>409.25</td>\n",
              "      <td>2419.25</td>\n",
              "      <td>6106.30</td>\n",
              "      <td>884.25</td>\n",
              "      <td>322.35</td>\n",
              "      <td>2946.55</td>\n",
              "      <td>637.45</td>\n",
              "      <td>1126.95</td>\n",
              "      <td>2991.80</td>\n",
              "      <td>370.60</td>\n",
              "      <td>3557.20</td>\n",
              "      <td>825.05</td>\n",
              "      <td>1056.20</td>\n",
              "      <td>2520.10</td>\n",
              "      <td>76.65</td>\n",
              "      <td>820.25</td>\n",
              "      <td>1179.45</td>\n",
              "      <td>246.75</td>\n",
              "      <td>309.60</td>\n",
              "      <td>441.55</td>\n",
              "      <td>925.50</td>\n",
              "      <td>1050.80</td>\n",
              "      <td>1026.35</td>\n",
              "      <td>691.65</td>\n",
              "      <td>1206.40</td>\n",
              "      <td>1197.45</td>\n",
              "      <td>137.85</td>\n",
              "      <td>226.60</td>\n",
              "      <td>138.10</td>\n",
              "      <td>1013.25</td>\n",
              "      <td>209.55</td>\n",
              "      <td>783.80</td>\n",
              "      <td>2371.25</td>\n",
              "      <td>343.55</td>\n",
              "      <td>249.90</td>\n",
              "      <td>510.80</td>\n",
              "      <td>344.05</td>\n",
              "      <td>417.40</td>\n",
              "      <td>2667.70</td>\n",
              "      <td>549.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>499.45</td>\n",
              "      <td>2793.85</td>\n",
              "      <td>664.45</td>\n",
              "      <td>3492.65</td>\n",
              "      <td>5119.00</td>\n",
              "      <td>392.00</td>\n",
              "      <td>514.00</td>\n",
              "      <td>3551.10</td>\n",
              "      <td>827.25</td>\n",
              "      <td>3842.10</td>\n",
              "      <td>5286.90</td>\n",
              "      <td>129.30</td>\n",
              "      <td>961.30</td>\n",
              "      <td>991.35</td>\n",
              "      <td>1426.70</td>\n",
              "      <td>3067.20</td>\n",
              "      <td>250.30</td>\n",
              "      <td>2450.55</td>\n",
              "      <td>2651.85</td>\n",
              "      <td>537.25</td>\n",
              "      <td>211.45</td>\n",
              "      <td>93.00</td>\n",
              "      <td>921.65</td>\n",
              "      <td>1293.80</td>\n",
              "      <td>395.25</td>\n",
              "      <td>1959.75</td>\n",
              "      <td>1306.30</td>\n",
              "      <td>740.10</td>\n",
              "      <td>97.60</td>\n",
              "      <td>94.95</td>\n",
              "      <td>188.00</td>\n",
              "      <td>1966.10</td>\n",
              "      <td>281.75</td>\n",
              "      <td>603.45</td>\n",
              "      <td>3093.00</td>\n",
              "      <td>193.20</td>\n",
              "      <td>680.55</td>\n",
              "      <td>1003.85</td>\n",
              "      <td>1570.95</td>\n",
              "      <td>471.25</td>\n",
              "      <td>5341.20</td>\n",
              "      <td>406.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>496.80</td>\n",
              "      <td>2805.35</td>\n",
              "      <td>654.25</td>\n",
              "      <td>3462.70</td>\n",
              "      <td>5030.30</td>\n",
              "      <td>387.30</td>\n",
              "      <td>525.30</td>\n",
              "      <td>3539.70</td>\n",
              "      <td>824.80</td>\n",
              "      <td>3879.85</td>\n",
              "      <td>5288.30</td>\n",
              "      <td>134.00</td>\n",
              "      <td>984.60</td>\n",
              "      <td>978.20</td>\n",
              "      <td>1420.55</td>\n",
              "      <td>3083.55</td>\n",
              "      <td>259.05</td>\n",
              "      <td>2417.30</td>\n",
              "      <td>2638.85</td>\n",
              "      <td>546.70</td>\n",
              "      <td>205.40</td>\n",
              "      <td>94.10</td>\n",
              "      <td>922.35</td>\n",
              "      <td>1282.10</td>\n",
              "      <td>401.70</td>\n",
              "      <td>1970.40</td>\n",
              "      <td>1314.00</td>\n",
              "      <td>736.10</td>\n",
              "      <td>97.85</td>\n",
              "      <td>96.95</td>\n",
              "      <td>196.15</td>\n",
              "      <td>1914.25</td>\n",
              "      <td>285.05</td>\n",
              "      <td>605.30</td>\n",
              "      <td>3051.50</td>\n",
              "      <td>195.40</td>\n",
              "      <td>683.80</td>\n",
              "      <td>997.15</td>\n",
              "      <td>1572.60</td>\n",
              "      <td>472.40</td>\n",
              "      <td>5448.35</td>\n",
              "      <td>406.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>513.85</td>\n",
              "      <td>2792.25</td>\n",
              "      <td>671.10</td>\n",
              "      <td>3437.95</td>\n",
              "      <td>5081.00</td>\n",
              "      <td>392.20</td>\n",
              "      <td>545.25</td>\n",
              "      <td>3552.80</td>\n",
              "      <td>826.55</td>\n",
              "      <td>3803.05</td>\n",
              "      <td>5270.90</td>\n",
              "      <td>133.45</td>\n",
              "      <td>993.85</td>\n",
              "      <td>962.55</td>\n",
              "      <td>1416.25</td>\n",
              "      <td>3055.25</td>\n",
              "      <td>272.90</td>\n",
              "      <td>2368.85</td>\n",
              "      <td>2661.35</td>\n",
              "      <td>541.10</td>\n",
              "      <td>202.80</td>\n",
              "      <td>94.50</td>\n",
              "      <td>952.05</td>\n",
              "      <td>1262.15</td>\n",
              "      <td>405.40</td>\n",
              "      <td>1952.40</td>\n",
              "      <td>1338.95</td>\n",
              "      <td>744.40</td>\n",
              "      <td>97.00</td>\n",
              "      <td>97.90</td>\n",
              "      <td>197.05</td>\n",
              "      <td>1911.15</td>\n",
              "      <td>287.70</td>\n",
              "      <td>601.90</td>\n",
              "      <td>3032.80</td>\n",
              "      <td>196.75</td>\n",
              "      <td>722.80</td>\n",
              "      <td>994.75</td>\n",
              "      <td>1542.35</td>\n",
              "      <td>482.50</td>\n",
              "      <td>5397.95</td>\n",
              "      <td>406.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>517.00</td>\n",
              "      <td>2844.70</td>\n",
              "      <td>672.70</td>\n",
              "      <td>3529.15</td>\n",
              "      <td>5082.00</td>\n",
              "      <td>400.15</td>\n",
              "      <td>540.25</td>\n",
              "      <td>3575.25</td>\n",
              "      <td>838.70</td>\n",
              "      <td>3859.15</td>\n",
              "      <td>5338.25</td>\n",
              "      <td>132.75</td>\n",
              "      <td>1004.30</td>\n",
              "      <td>994.65</td>\n",
              "      <td>1431.65</td>\n",
              "      <td>3161.10</td>\n",
              "      <td>268.20</td>\n",
              "      <td>2391.20</td>\n",
              "      <td>2657.50</td>\n",
              "      <td>542.05</td>\n",
              "      <td>201.50</td>\n",
              "      <td>96.15</td>\n",
              "      <td>939.80</td>\n",
              "      <td>1312.10</td>\n",
              "      <td>402.85</td>\n",
              "      <td>1970.70</td>\n",
              "      <td>1373.40</td>\n",
              "      <td>770.50</td>\n",
              "      <td>100.15</td>\n",
              "      <td>100.65</td>\n",
              "      <td>203.55</td>\n",
              "      <td>1933.70</td>\n",
              "      <td>286.00</td>\n",
              "      <td>620.80</td>\n",
              "      <td>3120.90</td>\n",
              "      <td>198.15</td>\n",
              "      <td>713.15</td>\n",
              "      <td>1051.10</td>\n",
              "      <td>1548.60</td>\n",
              "      <td>503.65</td>\n",
              "      <td>5591.75</td>\n",
              "      <td>430.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>508.35</td>\n",
              "      <td>2849.30</td>\n",
              "      <td>666.95</td>\n",
              "      <td>3617.35</td>\n",
              "      <td>4984.25</td>\n",
              "      <td>401.75</td>\n",
              "      <td>547.15</td>\n",
              "      <td>3612.85</td>\n",
              "      <td>856.85</td>\n",
              "      <td>3822.40</td>\n",
              "      <td>5416.80</td>\n",
              "      <td>135.70</td>\n",
              "      <td>1004.30</td>\n",
              "      <td>1055.10</td>\n",
              "      <td>1451.45</td>\n",
              "      <td>3197.70</td>\n",
              "      <td>264.70</td>\n",
              "      <td>2429.10</td>\n",
              "      <td>2751.20</td>\n",
              "      <td>544.70</td>\n",
              "      <td>202.50</td>\n",
              "      <td>96.00</td>\n",
              "      <td>929.10</td>\n",
              "      <td>1376.20</td>\n",
              "      <td>399.05</td>\n",
              "      <td>1938.15</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>789.10</td>\n",
              "      <td>99.00</td>\n",
              "      <td>102.55</td>\n",
              "      <td>203.55</td>\n",
              "      <td>1897.25</td>\n",
              "      <td>282.50</td>\n",
              "      <td>621.00</td>\n",
              "      <td>3176.45</td>\n",
              "      <td>220.65</td>\n",
              "      <td>695.65</td>\n",
              "      <td>1077.60</td>\n",
              "      <td>1563.90</td>\n",
              "      <td>497.45</td>\n",
              "      <td>5623.70</td>\n",
              "      <td>446.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1245 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ADANIPORTS  ASIANPAINT  AXISBANK  ...     UPL  ULTRACEMCO   WIPRO\n",
              "0         267.55      878.75    449.90  ...  440.35     2824.00  556.45\n",
              "1         257.95      880.80    438.40  ...  433.85     2749.15  557.70\n",
              "2         255.70      900.65    436.45  ...  435.30     2733.05  556.90\n",
              "3         251.80      886.40    430.70  ...  443.35     2735.30  555.10\n",
              "4         245.00      872.35    409.25  ...  417.40     2667.70  549.85\n",
              "...          ...         ...       ...  ...     ...         ...     ...\n",
              "1240      499.45     2793.85    664.45  ...  471.25     5341.20  406.30\n",
              "1241      496.80     2805.35    654.25  ...  472.40     5448.35  406.40\n",
              "1242      513.85     2792.25    671.10  ...  482.50     5397.95  406.75\n",
              "1243      517.00     2844.70    672.70  ...  503.65     5591.75  430.20\n",
              "1244      508.35     2849.30    666.95  ...  497.45     5623.70  446.80\n",
              "\n",
              "[1245 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfmGW-eD2p73"
      },
      "source": [
        "alldata=x   #the historical dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuN2s7Ogwz1k"
      },
      "source": [
        "timesteps=30                                     #lstm hyperparameters \"Subject to be tuned\"\r\n",
        "epoch=50\r\n",
        "batchSize=64\r\n",
        "neurons=50\r\n",
        "after2020=len(alldata)-1238                      #number of days after 31-12-2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcGk0vzctud"
      },
      "source": [
        "def create_dataset(dataset, time_step=1):         # convert an array of values into a dataset matrix which will be used to train the lstm model.\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-time_step):\n",
        "\t\ta = dataset[i:(i+time_step), 0]               #i=0, 0,1,2,3-----99   100\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + time_step, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4weFefCyPmt"
      },
      "source": [
        "def forcast(df1, timesteps, epoch, batchSize, neurons, after2020):             \r\n",
        "  scaler=MinMaxScaler(feature_range=(0,1))   \r\n",
        "  df1=scaler.fit_transform(np.array(df1).reshape(-1,1))           #minmax scalar transformation of data\r\n",
        "\r\n",
        "  training_size=int(len(df1)-after2020)                                \r\n",
        "  train_data=df1[0:training_size,:]                               #getting the training data\r\n",
        "  inpdata=df1[training_size-timesteps:len(df1),:1]                #getting the data from 01-01-2021 onwards\r\n",
        "  val_data=train_data[int(training_size*0.75):len(train_data),:1] #validation data (25% of training data)\r\n",
        "\r\n",
        "  time_step = timesteps                                           #reshape into X=t,t+1,t+2,t+3,........t+\"timestep-1\" and Y=t+\"timestep\"\r\n",
        "  X_train, y_train = create_dataset(train_data, time_step)\r\n",
        "  x_inp, y_inp = create_dataset(inpdata, timesteps)\r\n",
        "  x_val, y_val = create_dataset(val_data,timesteps)\r\n",
        "\r\n",
        "  X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1) \r\n",
        "  x_inp = x_inp.reshape(x_inp.shape[0],x_inp.shape[1] , 1)        #reshape input to be [samples, time steps, features] which is required for LSTM\r\n",
        "  x_val = x_val.reshape(x_val.shape[0],x_val.shape[1] , 1)\r\n",
        "\r\n",
        "  # initialising stacked lstm\r\n",
        "  model=Sequential()\r\n",
        "  model.add(LSTM(neurons,return_sequences=True,input_shape=(time_step,1)))\r\n",
        "  model.add(LSTM(neurons,return_sequences=True))\r\n",
        "  model.add(LSTM(neurons))\r\n",
        "  model.add(Dense(1))\r\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\r\n",
        "\r\n",
        "  model.fit(X_train,y_train,validation_data=(x_val,y_val),epochs=epoch,batch_size=batchSize,verbose=1)     # training of the model\r\n",
        "  \r\n",
        "  out_predict=model.predict(x_inp)                      #dynamic prediction of the stock's closing price from 01-01-2021 onwards\r\n",
        "\r\n",
        "  v=math.sqrt(mean_squared_error(y_inp,out_predict))    # RMSE value calculated here\r\n",
        "\r\n",
        "  lst=[]\r\n",
        "  for i in out_predict:\r\n",
        "    lst.append(i)\r\n",
        "\r\n",
        "  df3=train_data.tolist()\r\n",
        "  df3.extend(lst)                                       #appending the data, forcasted from 01-01-2021 onwards, to the data till 31-12-2020\r\n",
        "  df3=scaler.inverse_transform(df3).tolist()\r\n",
        "\r\n",
        "  return pd.DataFrame(df3),v\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcpJPAnHwTFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba5ed5c-ac78-45dd-b14e-12d677828774"
      },
      "source": [
        "rmse=[]\r\n",
        "fdata=pd.DataFrame()\r\n",
        "for i in alldata:                                   # this for loop will be iterated for 42 times i.e. for each column of the historical dataset\r\n",
        "  temp=alldata[i]\r\n",
        "  ftemp,trmse=forcast(temp, timesteps, epoch, batchSize, neurons, after2020)    #hyperparameters are provided as input here\r\n",
        "  fdata = pd.concat([fdata,ftemp],axis = 1)\r\n",
        "  rmse.append(trmse)\r\n",
        "fdata.columns=stonks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 1s 60ms/step - loss: 0.0026 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0027 - val_loss: 0.0041\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 60ms/step - loss: 0.0024 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0021 - val_loss: 0.0036\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0021 - val_loss: 0.0031\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 60ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 125ms/step - loss: 0.0387 - val_loss: 0.0030\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0081 - val_loss: 0.0017\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0069 - val_loss: 0.0014\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0045 - val_loss: 0.0017\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0030 - val_loss: 0.0017\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0028 - val_loss: 8.9470e-04\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0043 - val_loss: 0.0019\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 60ms/step - loss: 0.0029 - val_loss: 7.8089e-04\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0022 - val_loss: 0.0012\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0025 - val_loss: 8.6558e-04\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0025 - val_loss: 9.9205e-04\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0041 - val_loss: 7.1661e-04\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0032 - val_loss: 7.5647e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0035 - val_loss: 0.0014\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0018 - val_loss: 5.4083e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0012 - val_loss: 5.9580e-04\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0021 - val_loss: 8.5146e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.8080e-04 - val_loss: 5.9767e-04\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0017 - val_loss: 5.6914e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0015 - val_loss: 5.3432e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 8.7232e-04 - val_loss: 8.5453e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0033 - val_loss: 5.6350e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0019 - val_loss: 5.0313e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 7.7636e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 8.1763e-04 - val_loss: 4.5228e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0016 - val_loss: 5.4110e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 5.8795e-04 - val_loss: 5.9870e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0020 - val_loss: 4.9420e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0010 - val_loss: 5.0251e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0019 - val_loss: 6.7150e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 5.1833e-04 - val_loss: 3.8066e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0012 - val_loss: 5.2567e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 4.6153e-04 - val_loss: 5.2781e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0019 - val_loss: 4.1903e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 5.5007e-04 - val_loss: 3.4225e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 7.2970e-04 - val_loss: 6.4796e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 4.8395e-04 - val_loss: 4.4799e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0011 - val_loss: 4.0825e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 8.2316e-04 - val_loss: 3.5959e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0014 - val_loss: 4.4896e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0011 - val_loss: 3.7372e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0013 - val_loss: 3.7486e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.2122e-04 - val_loss: 5.8767e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 7.1673e-04 - val_loss: 3.0630e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0017 - val_loss: 3.9056e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 6.9331e-04 - val_loss: 4.3668e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0017 - val_loss: 4.6795e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0014 - val_loss: 4.2238e-04\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8c015f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 124ms/step - loss: 0.0655 - val_loss: 0.0015\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0063 - val_loss: 0.0017\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0041 - val_loss: 0.0017\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0030 - val_loss: 0.0017\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0033 - val_loss: 0.0015\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0026 - val_loss: 0.0012\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0034 - val_loss: 0.0013\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0032 - val_loss: 0.0012\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0026 - val_loss: 0.0015\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0024 - val_loss: 0.0012\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0025 - val_loss: 0.0011\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0023 - val_loss: 0.0010\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 0.0010\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0013 - val_loss: 9.9257e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0018 - val_loss: 9.5621e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0012 - val_loss: 8.4442e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 8.4434e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0017 - val_loss: 8.4607e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0018 - val_loss: 7.7943e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - val_loss: 7.9281e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0016 - val_loss: 8.3765e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0016 - val_loss: 8.4015e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0011 - val_loss: 6.8659e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 9.5305e-04 - val_loss: 6.7980e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0015 - val_loss: 7.6209e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0012 - val_loss: 6.5888e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - val_loss: 6.8290e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0019 - val_loss: 6.9597e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0010 - val_loss: 6.3468e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0017 - val_loss: 6.3190e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0012 - val_loss: 5.9338e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0013 - val_loss: 5.9079e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0012 - val_loss: 5.7659e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0012 - val_loss: 6.3013e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - val_loss: 5.7064e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 9.1728e-04 - val_loss: 5.7216e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0010 - val_loss: 5.7751e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - val_loss: 5.7965e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 7.7638e-04 - val_loss: 5.6481e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0012 - val_loss: 5.4183e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 6.7922e-04 - val_loss: 5.2509e-04\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe60948c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 123ms/step - loss: 0.0651 - val_loss: 0.0206\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0088 - val_loss: 0.0109\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0058 - val_loss: 0.0104\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0052 - val_loss: 0.0094\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0049 - val_loss: 0.0085\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0045 - val_loss: 0.0074\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0040 - val_loss: 0.0063\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0043 - val_loss: 0.0069\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0037 - val_loss: 0.0053\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0034 - val_loss: 0.0049\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0034 - val_loss: 0.0070\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0033 - val_loss: 0.0056\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0032 - val_loss: 0.0043\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0029 - val_loss: 0.0041\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0029 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0025 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.0024 - val_loss: 0.0042\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0029 - val_loss: 0.0038\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0023 - val_loss: 0.0041\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0024 - val_loss: 0.0044\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0026 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0024 - val_loss: 0.0039\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0025 - val_loss: 0.0033\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0017 - val_loss: 0.0033\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 62ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0014 - val_loss: 0.0030\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe633f290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 8s 131ms/step - loss: 0.0887 - val_loss: 0.0029\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0076 - val_loss: 0.0017\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 0.0034 - val_loss: 0.0016\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0033 - val_loss: 0.0015\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0036 - val_loss: 0.0013\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0029 - val_loss: 0.0013\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0023 - val_loss: 0.0012\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0033 - val_loss: 0.0011\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0026 - val_loss: 0.0010\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0020 - val_loss: 9.8941e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 0.0012\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0022 - val_loss: 9.0186e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0017 - val_loss: 9.2057e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0028 - val_loss: 0.0011\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0030 - val_loss: 9.3990e-04\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 9.1514e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0012 - val_loss: 7.5598e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 7.3394e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0014 - val_loss: 8.2503e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 7.0243e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 7.5555e-04 - val_loss: 6.9978e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.1778e-04 - val_loss: 7.3657e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0018 - val_loss: 7.5716e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 6.6415e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0010 - val_loss: 6.3794e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 6.5803e-04 - val_loss: 8.2883e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.9591e-04 - val_loss: 7.0849e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0021 - val_loss: 6.4505e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.6773e-04 - val_loss: 5.9329e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 8.8676e-04 - val_loss: 5.8485e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0012 - val_loss: 6.6915e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 5.8040e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0013 - val_loss: 6.4428e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0011 - val_loss: 5.7513e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 8.5070e-04 - val_loss: 5.6523e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 7.7674e-04 - val_loss: 5.5301e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.0329e-04 - val_loss: 7.0335e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 8.6795e-04 - val_loss: 6.5062e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.7225e-04 - val_loss: 5.1238e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.3342e-04 - val_loss: 7.9777e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0018 - val_loss: 5.2608e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0011 - val_loss: 5.0675e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 4.6730e-04 - val_loss: 4.8342e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.0587e-04 - val_loss: 5.6937e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.9176e-04 - val_loss: 4.8656e-04\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe4abce60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 148ms/step - loss: 0.0709 - val_loss: 0.0238\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0066 - val_loss: 0.0108\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0040 - val_loss: 0.0077\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0031 - val_loss: 0.0081\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0029 - val_loss: 0.0073\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0028 - val_loss: 0.0078\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0027 - val_loss: 0.0068\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0027 - val_loss: 0.0062\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 0.0074\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0025 - val_loss: 0.0056\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0021 - val_loss: 0.0055\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0020 - val_loss: 0.0062\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0023 - val_loss: 0.0049\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0022 - val_loss: 0.0048\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0017 - val_loss: 0.0045\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0018 - val_loss: 0.0043\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0016 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0014 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0014 - val_loss: 0.0039\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0015 - val_loss: 0.0035\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 0.0033\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0014 - val_loss: 0.0032\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 0.0032\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0014 - val_loss: 0.0032\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0015 - val_loss: 0.0037\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0010 - val_loss: 0.0030\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0010 - val_loss: 0.0029\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.1671e-04 - val_loss: 0.0024\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 63ms/step - loss: 0.0011 - val_loss: 0.0024\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.1656e-04 - val_loss: 0.0024\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.7305e-04 - val_loss: 0.0024\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 0.0040\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.9211e-04 - val_loss: 0.0023\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 8.9855e-04 - val_loss: 0.0021\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.3222e-04 - val_loss: 0.0021\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 9.3740e-04 - val_loss: 0.0020\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 7.8473e-04 - val_loss: 0.0023\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe633fb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 129ms/step - loss: 0.0696 - val_loss: 0.0305\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0033 - val_loss: 0.0064\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 0.0032\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.8692e-04 - val_loss: 0.0032\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.5425e-04 - val_loss: 0.0031\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.5350e-04 - val_loss: 0.0031\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 8.0797e-04 - val_loss: 0.0030\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.2413e-04 - val_loss: 0.0029\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.5405e-04 - val_loss: 0.0028\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 8.6400e-04 - val_loss: 0.0030\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 8.4878e-04 - val_loss: 0.0027\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 8.4430e-04 - val_loss: 0.0025\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.9193e-04 - val_loss: 0.0024\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 7.4840e-04 - val_loss: 0.0023\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.1905e-04 - val_loss: 0.0023\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.7807e-04 - val_loss: 0.0023\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.6788e-04 - val_loss: 0.0021\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.0123e-04 - val_loss: 0.0020\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.5652e-04 - val_loss: 0.0038\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 7.9429e-04 - val_loss: 0.0022\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.2944e-04 - val_loss: 0.0019\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 5.3295e-04 - val_loss: 0.0018\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.6078e-04 - val_loss: 0.0017\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 5.2732e-04 - val_loss: 0.0018\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 5.5337e-04 - val_loss: 0.0016\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 4.9435e-04 - val_loss: 0.0016\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 6.1429e-04 - val_loss: 0.0017\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 5.8818e-04 - val_loss: 0.0015\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.5018e-04 - val_loss: 0.0015\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 5.9479e-04 - val_loss: 0.0015\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 4.6531e-04 - val_loss: 0.0016\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.6327e-04 - val_loss: 0.0013\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 4.8465e-04 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 5.6320e-04 - val_loss: 0.0018\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 4.9678e-04 - val_loss: 0.0012\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 4.3142e-04 - val_loss: 0.0012\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.1514e-04 - val_loss: 0.0013\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 5.2260e-04 - val_loss: 0.0011\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.5975e-04 - val_loss: 0.0017\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 5.2901e-04 - val_loss: 0.0017\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.5548e-04 - val_loss: 0.0011\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 4.2163e-04 - val_loss: 0.0013\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.2481e-04 - val_loss: 0.0011\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 3.5913e-04 - val_loss: 0.0013\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 4.1123e-04 - val_loss: 0.0012\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 4.5364e-04 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 3.9286e-04 - val_loss: 9.5654e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 4.0403e-04 - val_loss: 0.0011\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 3.6359e-04 - val_loss: 9.3076e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 3.2112e-04 - val_loss: 0.0013\n",
            "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe60a1440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 127ms/step - loss: 0.0546 - val_loss: 0.0325\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0050 - val_loss: 0.0081\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0029 - val_loss: 0.0064\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0021 - val_loss: 0.0060\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0022 - val_loss: 0.0058\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0023 - val_loss: 0.0053\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0020 - val_loss: 0.0053\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0019 - val_loss: 0.0052\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0020 - val_loss: 0.0047\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0019 - val_loss: 0.0042\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0018 - val_loss: 0.0043\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0015 - val_loss: 0.0031\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0010 - val_loss: 0.0022\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 8.9723e-04 - val_loss: 0.0021\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.6534e-04 - val_loss: 0.0020\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.9025e-04 - val_loss: 0.0019\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.6297e-04 - val_loss: 0.0023\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.5929e-04 - val_loss: 0.0019\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.2639e-04 - val_loss: 0.0020\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.4193e-04 - val_loss: 0.0021\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.5410e-04 - val_loss: 0.0018\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 8.2261e-04 - val_loss: 0.0018\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.6977e-04 - val_loss: 0.0021\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.4375e-04 - val_loss: 0.0017\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.7812e-04 - val_loss: 0.0016\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 7.2541e-04 - val_loss: 0.0019\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.9464e-04 - val_loss: 0.0016\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.0419e-04 - val_loss: 0.0025\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.3860e-04 - val_loss: 0.0028\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.6474e-04 - val_loss: 0.0017\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 7.8546e-04 - val_loss: 0.0015\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.8554e-04 - val_loss: 0.0014\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.7341e-04 - val_loss: 0.0015\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.5083e-04 - val_loss: 0.0016\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.1135e-04 - val_loss: 0.0032\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.4862e-04 - val_loss: 0.0015\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 6.8936e-04 - val_loss: 0.0013\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 6.0700e-04 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 6.5295e-04 - val_loss: 0.0021\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.3220e-04 - val_loss: 0.0012\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 5.7362e-04 - val_loss: 0.0014\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 5.4917e-04 - val_loss: 0.0016\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 6.2976e-04 - val_loss: 0.0012\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.7588e-04 - val_loss: 0.0011\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 6.2437e-04 - val_loss: 0.0012\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe49d93b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 125ms/step - loss: 0.1100 - val_loss: 0.0096\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0091 - val_loss: 0.0023\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0040 - val_loss: 6.0183e-04\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0035 - val_loss: 6.0247e-04\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0026 - val_loss: 4.8909e-04\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0027 - val_loss: 4.6094e-04\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0025 - val_loss: 4.7612e-04\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 4.3378e-04\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 4.0433e-04\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0030 - val_loss: 3.9230e-04\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0024 - val_loss: 3.6852e-04\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 0.0021 - val_loss: 3.5801e-04\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0020 - val_loss: 3.4715e-04\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0018 - val_loss: 3.5384e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0023 - val_loss: 3.2484e-04\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0022 - val_loss: 3.3613e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0021 - val_loss: 3.1705e-04\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0020 - val_loss: 3.0341e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0021 - val_loss: 3.0091e-04\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0015 - val_loss: 2.9115e-04\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0019 - val_loss: 2.8858e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0018 - val_loss: 2.8008e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0017 - val_loss: 2.8481e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0014 - val_loss: 2.6937e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0021 - val_loss: 2.8680e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0015 - val_loss: 2.4802e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 2.5832e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0016 - val_loss: 2.4422e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0015 - val_loss: 2.3321e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 2.3633e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 2.2983e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 2.1994e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 2.2339e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 2.2193e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 2.1070e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0014 - val_loss: 2.0723e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 2.0687e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0010 - val_loss: 2.1980e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 2.1725e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 1.9547e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0010 - val_loss: 2.0641e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0010 - val_loss: 1.9564e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 1.9641e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 1.9122e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 1.8004e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 9.4743e-04 - val_loss: 1.7720e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.0609e-04 - val_loss: 1.8405e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.4742e-04 - val_loss: 1.7381e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 64ms/step - loss: 9.9240e-04 - val_loss: 1.7593e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 9.1826e-04 - val_loss: 1.7651e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe9711440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 137ms/step - loss: 0.0505 - val_loss: 3.2777e-04\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0066 - val_loss: 5.2405e-04\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0052 - val_loss: 1.4831e-04\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0051 - val_loss: 1.2602e-04\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0031 - val_loss: 1.1690e-04\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0035 - val_loss: 1.9412e-04\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0035 - val_loss: 1.6495e-04\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0055 - val_loss: 1.9383e-04\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0032 - val_loss: 7.5189e-05\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0016 - val_loss: 7.7213e-05\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0027 - val_loss: 7.2095e-05\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 7.7301e-05\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0015 - val_loss: 6.9291e-05\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 1.7902e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0016 - val_loss: 7.5545e-05\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0019 - val_loss: 7.0370e-05\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.3200e-04 - val_loss: 9.0750e-05\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0010 - val_loss: 1.2953e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0010 - val_loss: 7.3274e-05\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0014 - val_loss: 6.2322e-05\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 8.5752e-04 - val_loss: 7.3269e-05\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0017 - val_loss: 6.2553e-05\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 7.2841e-04 - val_loss: 8.0378e-05\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0023 - val_loss: 5.9452e-05\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 6.1667e-05\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 7.2172e-04 - val_loss: 8.5293e-05\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0026 - val_loss: 9.8102e-05\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.0785e-04 - val_loss: 8.6062e-05\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.6336e-04 - val_loss: 1.4739e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 8.8577e-04 - val_loss: 7.8203e-05\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0028 - val_loss: 5.8375e-05\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 6.6821e-05\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 6.8670e-04 - val_loss: 5.6396e-05\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 6.1183e-05\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 7.8661e-05\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.9852e-04 - val_loss: 5.7623e-05\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 2.7172e-04 - val_loss: 6.2703e-05\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0017 - val_loss: 9.7933e-05\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 4.7687e-04 - val_loss: 8.5118e-05\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 1.6249e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 6.5118e-05\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 5.0028e-05\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.7462e-04 - val_loss: 6.2031e-05\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 5.7663e-05\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.2287e-04 - val_loss: 5.4153e-05\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 7.4486e-04 - val_loss: 4.9670e-05\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 6.7234e-04 - val_loss: 6.8971e-05\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0015 - val_loss: 1.2830e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 7.4337e-04 - val_loss: 4.9078e-05\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 6.6877e-04 - val_loss: 5.6769e-05\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fedf940e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 132ms/step - loss: 0.1657 - val_loss: 0.0088\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0130 - val_loss: 0.0076\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0090 - val_loss: 0.0060\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0063 - val_loss: 0.0040\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0064 - val_loss: 0.0039\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0066 - val_loss: 0.0035\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0053 - val_loss: 0.0036\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0061 - val_loss: 0.0032\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0055 - val_loss: 0.0031\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0044 - val_loss: 0.0028\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0058 - val_loss: 0.0025\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0068 - val_loss: 0.0026\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0051 - val_loss: 0.0023\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0048 - val_loss: 0.0021\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0039 - val_loss: 0.0020\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0038 - val_loss: 0.0020\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0037 - val_loss: 0.0019\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0035 - val_loss: 0.0019\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0035 - val_loss: 0.0017\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0027 - val_loss: 0.0016\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0033 - val_loss: 0.0015\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 0.0013\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0018 - val_loss: 0.0013\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0021 - val_loss: 0.0012\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 9.9951e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0011\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 9.8050e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe37e58c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 127ms/step - loss: 0.1166 - val_loss: 0.0042\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0118 - val_loss: 0.0030\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0046 - val_loss: 0.0024\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0043 - val_loss: 0.0023\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0035 - val_loss: 0.0023\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0031 - val_loss: 0.0020\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0034 - val_loss: 0.0017\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0027 - val_loss: 0.0014\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0023 - val_loss: 0.0014\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0020 - val_loss: 0.0012\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0019 - val_loss: 0.0012\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0017 - val_loss: 0.0011\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 7.6406e-04 - val_loss: 9.8772e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.5024e-04 - val_loss: 9.6989e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0019 - val_loss: 0.0010\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0019 - val_loss: 9.6637e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 9.4409e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 9.2463e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 6.9990e-04 - val_loss: 9.3133e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0020 - val_loss: 0.0010\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 9.0453e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 9.3033e-04 - val_loss: 8.7749e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 8.6866e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.7506e-04 - val_loss: 8.4661e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 8.4049e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8cc44d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 126ms/step - loss: 0.1987 - val_loss: 0.0127\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0140 - val_loss: 0.0081\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0059 - val_loss: 0.0063\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0041 - val_loss: 0.0054\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0031 - val_loss: 0.0052\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0031 - val_loss: 0.0051\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0030 - val_loss: 0.0050\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0031 - val_loss: 0.0049\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0029 - val_loss: 0.0047\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0028 - val_loss: 0.0045\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0028 - val_loss: 0.0044\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0027 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0027 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0028 - val_loss: 0.0039\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0023 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0025 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0028 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0024 - val_loss: 0.0034\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0023 - val_loss: 0.0029\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0021 - val_loss: 0.0028\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 9.4550e-04 - val_loss: 0.0015\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe859f4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 129ms/step - loss: 0.2247 - val_loss: 0.0157\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0114 - val_loss: 0.0103\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0054 - val_loss: 0.0092\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0040 - val_loss: 0.0084\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0033 - val_loss: 0.0078\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0033 - val_loss: 0.0076\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 65ms/step - loss: 0.0033 - val_loss: 0.0073\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0033 - val_loss: 0.0071\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0030 - val_loss: 0.0067\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0028 - val_loss: 0.0060\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0056\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0026 - val_loss: 0.0054\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0028 - val_loss: 0.0051\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0027 - val_loss: 0.0048\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0024 - val_loss: 0.0045\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0022 - val_loss: 0.0041\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0023 - val_loss: 0.0040\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0020 - val_loss: 0.0038\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 66ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe6051440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 151ms/step - loss: 0.1411 - val_loss: 0.0610\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0113 - val_loss: 0.0105\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0022 - val_loss: 0.0044\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 0.0045\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0017 - val_loss: 0.0049\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 0.0043\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 0.0053\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 0.0041\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0038\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0014 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0035\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 9.9250e-04 - val_loss: 0.0033\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0033\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0031\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0030\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 9.6437e-04 - val_loss: 0.0036\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0013 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.1004e-04 - val_loss: 0.0027\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 8.2560e-04 - val_loss: 0.0025\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0010 - val_loss: 0.0027\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 8.4111e-04 - val_loss: 0.0024\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 8.9684e-04 - val_loss: 0.0020\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 7.9482e-04 - val_loss: 0.0019\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 6.9795e-04 - val_loss: 0.0018\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 7.3629e-04 - val_loss: 0.0017\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 7.3060e-04 - val_loss: 0.0020\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 6.1635e-04 - val_loss: 0.0019\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 7.5243e-04 - val_loss: 0.0021\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 7.2345e-04 - val_loss: 0.0016\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 7.1421e-04 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 7.6245e-04 - val_loss: 0.0016\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 7.8347e-04 - val_loss: 0.0015\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 6.1245e-04 - val_loss: 0.0015\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 6.0798e-04 - val_loss: 0.0017\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe605f050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 154ms/step - loss: 0.1067 - val_loss: 0.0225\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0092 - val_loss: 0.0094\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0036 - val_loss: 0.0082\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0027 - val_loss: 0.0083\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0025 - val_loss: 0.0082\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0022 - val_loss: 0.0075\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 0.0072\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0022 - val_loss: 0.0069\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0021 - val_loss: 0.0060\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0021 - val_loss: 0.0053\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0018 - val_loss: 0.0047\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0024 - val_loss: 0.0046\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 0.0045\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 0.0039\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0041\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 0.0038\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0016 - val_loss: 0.0038\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0015 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0014 - val_loss: 0.0035\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0038\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0034\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0014 - val_loss: 0.0033\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0013 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0031\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0030\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0031\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0024\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0010 - val_loss: 0.0024\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0029\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0010 - val_loss: 0.0025\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0010 - val_loss: 0.0023\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe60514d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 130ms/step - loss: 0.0872 - val_loss: 0.0186\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0079 - val_loss: 0.0123\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0046 - val_loss: 0.0133\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0038 - val_loss: 0.0118\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0037 - val_loss: 0.0110\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0027 - val_loss: 0.0098\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0029 - val_loss: 0.0084\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0026 - val_loss: 0.0081\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0026 - val_loss: 0.0065\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0062\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 0.0060\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0024 - val_loss: 0.0056\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0021 - val_loss: 0.0057\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0025 - val_loss: 0.0053\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 0.0051\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0020 - val_loss: 0.0047\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0018 - val_loss: 0.0044\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0017 - val_loss: 0.0049\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0044\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 0.0037\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0015 - val_loss: 0.0036\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0017 - val_loss: 0.0035\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0039\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0014 - val_loss: 0.0032\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0028\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0028\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 9.9691e-04 - val_loss: 0.0022\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe50735f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 130ms/step - loss: 0.1124 - val_loss: 0.0066\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0093 - val_loss: 0.0063\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0061 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0045 - val_loss: 0.0041\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0045 - val_loss: 0.0036\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0038 - val_loss: 0.0033\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0029\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0026 - val_loss: 0.0030\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0025 - val_loss: 0.0027\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0032 - val_loss: 0.0026\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0028 - val_loss: 0.0022\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.8472e-04 - val_loss: 0.0014\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe6e19200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 131ms/step - loss: 0.0666 - val_loss: 0.0027\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0048 - val_loss: 2.0781e-04\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0037 - val_loss: 3.2905e-04\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0025 - val_loss: 1.4465e-04\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0035 - val_loss: 1.6246e-04\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0027 - val_loss: 1.0189e-04\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0033 - val_loss: 1.0225e-04\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0028 - val_loss: 2.1812e-04\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0033 - val_loss: 9.7310e-05\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 67ms/step - loss: 0.0021 - val_loss: 1.5389e-04\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0031 - val_loss: 1.0277e-04\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0019 - val_loss: 1.2567e-04\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0023 - val_loss: 9.9939e-05\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0026 - val_loss: 9.7485e-05\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 8.4765e-05\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0026 - val_loss: 1.4007e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0022 - val_loss: 1.1587e-04\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 1.2431e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 9.2230e-05\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 1.5104e-04\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 1.4112e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 8.6660e-05\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 7.4944e-05\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 7.7202e-05\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 7.8356e-05\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 1.2967e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 5.6213e-04 - val_loss: 5.8170e-05\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 6.5908e-05\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 8.1389e-05\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0012 - val_loss: 1.3126e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 6.0877e-05\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 8.9496e-04 - val_loss: 8.0559e-05\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0015 - val_loss: 1.2261e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 1.0518e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 7.5313e-05\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 9.2899e-05\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 5.6584e-04 - val_loss: 9.5714e-05\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 8.1309e-05\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.6725e-04 - val_loss: 6.0285e-05\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 5.4844e-04 - val_loss: 8.7505e-05\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 5.5134e-04 - val_loss: 4.7318e-05\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 7.8695e-04 - val_loss: 5.0037e-05\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0010 - val_loss: 4.9695e-05\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 6.3141e-04 - val_loss: 9.9306e-05\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 4.7877e-05\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 5.6897e-05\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 5.6118e-04 - val_loss: 4.8034e-05\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 8.9536e-04 - val_loss: 4.2748e-05\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 6.4545e-04 - val_loss: 4.4355e-05\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 7.0337e-04 - val_loss: 4.4060e-05\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8b75290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 139ms/step - loss: 0.1640 - val_loss: 0.0098\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0112 - val_loss: 0.0114\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0041 - val_loss: 0.0082\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0027 - val_loss: 0.0082\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0076\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0023 - val_loss: 0.0073\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0068\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0025 - val_loss: 0.0064\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0027 - val_loss: 0.0055\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0021 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0025 - val_loss: 0.0055\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0047\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0046\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0014 - val_loss: 0.0033\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0014 - val_loss: 0.0029\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0013 - val_loss: 0.0026\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.9484e-04 - val_loss: 0.0021\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.8300e-04 - val_loss: 0.0018\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 8.8208e-04 - val_loss: 0.0019\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 8.9748e-04 - val_loss: 0.0021\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.6831e-04 - val_loss: 0.0017\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 9.3848e-04 - val_loss: 0.0016\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.4944e-04 - val_loss: 0.0018\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 9.1372e-04 - val_loss: 0.0015\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.0541e-04 - val_loss: 0.0014\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.6463e-04 - val_loss: 0.0018\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 9.3319e-04 - val_loss: 0.0015\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe5031320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 129ms/step - loss: 0.1101 - val_loss: 0.0123\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0106 - val_loss: 0.0068\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0068 - val_loss: 0.0044\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0047 - val_loss: 0.0053\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0060 - val_loss: 0.0038\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0046 - val_loss: 0.0038\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0046 - val_loss: 0.0036\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0047 - val_loss: 0.0031\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0036 - val_loss: 0.0032\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0050 - val_loss: 0.0027\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0028 - val_loss: 0.0032\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0045 - val_loss: 0.0025\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0021 - val_loss: 0.0024\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0031 - val_loss: 0.0020\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0024 - val_loss: 0.0019\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0026 - val_loss: 0.0019\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0016 - val_loss: 0.0012\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 8.8614e-04 - val_loss: 0.0011\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 8.2876e-04 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe85f9170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 129ms/step - loss: 0.0695 - val_loss: 6.3140e-04\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0061 - val_loss: 6.2934e-04\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0038 - val_loss: 2.4673e-04\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0044 - val_loss: 2.0224e-04\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0032 - val_loss: 1.6553e-04\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0046 - val_loss: 2.5753e-04\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0021 - val_loss: 2.0346e-04\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0038 - val_loss: 1.7365e-04\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0038 - val_loss: 1.3464e-04\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0028 - val_loss: 1.6928e-04\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0041 - val_loss: 1.5703e-04\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0030 - val_loss: 1.9123e-04\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0024 - val_loss: 4.3020e-04\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0020 - val_loss: 1.3919e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0045 - val_loss: 1.2274e-04\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0032 - val_loss: 1.0741e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0010 - val_loss: 1.0495e-04\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0029 - val_loss: 1.0872e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0025 - val_loss: 1.1703e-04\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0021 - val_loss: 1.2227e-04\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 1.0064e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0017 - val_loss: 1.1626e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0017 - val_loss: 1.0164e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0023 - val_loss: 1.0084e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0026 - val_loss: 9.9967e-05\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0021 - val_loss: 1.0919e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 1.1997e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0022 - val_loss: 9.9762e-05\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 7.4625e-04 - val_loss: 1.1424e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0022 - val_loss: 1.0081e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 6.6911e-04 - val_loss: 1.0799e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0023 - val_loss: 9.6720e-05\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0019 - val_loss: 1.2276e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 9.5985e-05\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 9.2582e-05\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0016 - val_loss: 1.1551e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 3.2588e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 9.0974e-05\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0014 - val_loss: 9.7555e-05\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 5.9153e-04 - val_loss: 9.4648e-05\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0030 - val_loss: 9.4528e-05\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 5.9162e-04 - val_loss: 8.9006e-05\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 7.2709e-04 - val_loss: 8.2930e-05\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0015 - val_loss: 8.5680e-05\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 5.1051e-04 - val_loss: 8.6059e-05\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 8.4792e-05\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 5.7999e-04 - val_loss: 8.0041e-05\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 7.5153e-04 - val_loss: 7.9959e-05\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 6.0127e-04 - val_loss: 9.1011e-05\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0028 - val_loss: 7.8246e-05\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe7037440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 130ms/step - loss: 0.0949 - val_loss: 0.0253\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.0075 - val_loss: 0.0103\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0029 - val_loss: 0.0093\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0022 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0081\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0021 - val_loss: 0.0077\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0073\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0020 - val_loss: 0.0071\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0018 - val_loss: 0.0066\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0056\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0015 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0013 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0033\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.8573e-04 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.9085e-04 - val_loss: 0.0030\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.3772e-04 - val_loss: 0.0029\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.4109e-04 - val_loss: 0.0029\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 9.4401e-04 - val_loss: 0.0029\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0029\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 8.6558e-04 - val_loss: 0.0027\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 7.4417e-04 - val_loss: 0.0027\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 8.2314e-04 - val_loss: 0.0026\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 7.5911e-04 - val_loss: 0.0026\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.1232e-04 - val_loss: 0.0026\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 8.2038e-04 - val_loss: 0.0025\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 7.2428e-04 - val_loss: 0.0024\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 7.8281e-04 - val_loss: 0.0024\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 7.5314e-04 - val_loss: 0.0029\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 9.8326e-04 - val_loss: 0.0027\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 7.7750e-04 - val_loss: 0.0029\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 8.4992e-04 - val_loss: 0.0023\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 6.5373e-04 - val_loss: 0.0021\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 6.4170e-04 - val_loss: 0.0020\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 5.7255e-04 - val_loss: 0.0020\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 6.2530e-04 - val_loss: 0.0019\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 5.6004e-04 - val_loss: 0.0019\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 5.0924e-04 - val_loss: 0.0019\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 5.5402e-04 - val_loss: 0.0018\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 6.4165e-04 - val_loss: 0.0019\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 5.2630e-04 - val_loss: 0.0020\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 6.6363e-04 - val_loss: 0.0019\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe97660e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 144ms/step - loss: 0.1253 - val_loss: 0.0090\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0099 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0064 - val_loss: 0.0091\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0047 - val_loss: 0.0071\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0053 - val_loss: 0.0071\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0051 - val_loss: 0.0064\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0043 - val_loss: 0.0053\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0037 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0037 - val_loss: 0.0054\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0030 - val_loss: 0.0043\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0031 - val_loss: 0.0036\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0033 - val_loss: 0.0047\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0031 - val_loss: 0.0036\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0029 - val_loss: 0.0036\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0028 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0028 - val_loss: 0.0034\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0025 - val_loss: 0.0038\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0031\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0022 - val_loss: 0.0029\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0024 - val_loss: 0.0028\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0024 - val_loss: 0.0028\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0024 - val_loss: 0.0029\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8828320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 8s 135ms/step - loss: 0.1352 - val_loss: 0.0041\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0086 - val_loss: 0.0020\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0043 - val_loss: 0.0019\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0035 - val_loss: 0.0015\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0027 - val_loss: 0.0015\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0026 - val_loss: 0.0013\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0027 - val_loss: 0.0013\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0019 - val_loss: 0.0012\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0023 - val_loss: 0.0012\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0022 - val_loss: 0.0011\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0030 - val_loss: 0.0011\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0027 - val_loss: 0.0010\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0019 - val_loss: 0.0010\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0022 - val_loss: 0.0010\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0015 - val_loss: 0.0010\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0022 - val_loss: 9.7079e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0020 - val_loss: 9.7703e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0023 - val_loss: 9.2352e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0023 - val_loss: 0.0011\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0028 - val_loss: 9.8776e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0019 - val_loss: 9.4271e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0017 - val_loss: 9.0300e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0016 - val_loss: 8.8557e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0020 - val_loss: 8.6819e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0022 - val_loss: 8.4581e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0016 - val_loss: 8.5935e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0019 - val_loss: 7.8944e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 7.7306e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0015 - val_loss: 7.5902e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0012 - val_loss: 7.4813e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0016 - val_loss: 8.0989e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0014 - val_loss: 7.2214e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0015 - val_loss: 7.4860e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 6.9470e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 6.7403e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0018 - val_loss: 6.9710e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 6.4201e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 7.6363e-04 - val_loss: 6.2949e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0015 - val_loss: 6.6353e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 6.6095e-04 - val_loss: 6.1682e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 5.8639e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0011 - val_loss: 6.5142e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 9.4166e-04 - val_loss: 5.7428e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0017 - val_loss: 5.7900e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8828050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 131ms/step - loss: 0.1882 - val_loss: 0.0124\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0129 - val_loss: 0.0116\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0049 - val_loss: 0.0058\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0029 - val_loss: 0.0045\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0027 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0028 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0025 - val_loss: 0.0042\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0026 - val_loss: 0.0040\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0025 - val_loss: 0.0042\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0025 - val_loss: 0.0040\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0025 - val_loss: 0.0040\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0021 - val_loss: 0.0038\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0024 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0021 - val_loss: 0.0031\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe61fa560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 132ms/step - loss: 0.1236 - val_loss: 0.0075\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0114 - val_loss: 0.0043\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0039 - val_loss: 0.0012\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0026 - val_loss: 0.0013\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0025 - val_loss: 0.0012\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0017 - val_loss: 0.0012\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0024 - val_loss: 0.0012\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0021 - val_loss: 0.0011\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0019 - val_loss: 0.0011\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0017 - val_loss: 0.0011\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0021 - val_loss: 0.0011\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0021 - val_loss: 0.0010\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0018 - val_loss: 0.0011\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0019 - val_loss: 9.2242e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0018 - val_loss: 8.9869e-04\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 9.0017e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0014 - val_loss: 8.5813e-04\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0014 - val_loss: 8.7132e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0017 - val_loss: 8.8094e-04\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0016 - val_loss: 8.2845e-04\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 8.1515e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0014 - val_loss: 7.7910e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 7.7198e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0015 - val_loss: 8.8913e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 7.3487e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 7.0970e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 6.8388e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 9.1260e-04 - val_loss: 6.6185e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0010 - val_loss: 6.5261e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0012 - val_loss: 6.7426e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0012 - val_loss: 6.1842e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 6.5351e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 9.6140e-04 - val_loss: 6.5687e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 8.6219e-04 - val_loss: 5.5330e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 5.7501e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 9.8678e-04 - val_loss: 5.9191e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 9.4064e-04 - val_loss: 5.8554e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 8.8121e-04 - val_loss: 5.6906e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 9.4748e-04 - val_loss: 5.6120e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 5.6388e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 7.3921e-04 - val_loss: 5.8947e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 8.2545e-04 - val_loss: 5.3646e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 6.8942e-04 - val_loss: 5.0462e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0012 - val_loss: 5.3021e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 5.2187e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0010 - val_loss: 5.3074e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 7.8158e-04 - val_loss: 5.1017e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0010 - val_loss: 4.9335e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 7.8340e-04 - val_loss: 5.1789e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 8.4632e-04 - val_loss: 4.7867e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe6331050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 133ms/step - loss: 0.1648 - val_loss: 0.0112\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0118 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0065 - val_loss: 0.0099\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0050 - val_loss: 0.0101\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0049 - val_loss: 0.0098\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0041 - val_loss: 0.0093\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0044 - val_loss: 0.0096\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0044 - val_loss: 0.0088\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0039 - val_loss: 0.0089\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.0042 - val_loss: 0.0082\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0039 - val_loss: 0.0083\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0035 - val_loss: 0.0077\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0033 - val_loss: 0.0075\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0034 - val_loss: 0.0074\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0033 - val_loss: 0.0073\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0031 - val_loss: 0.0072\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0028 - val_loss: 0.0076\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0035 - val_loss: 0.0067\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0033 - val_loss: 0.0075\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0030 - val_loss: 0.0067\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0029 - val_loss: 0.0065\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0030 - val_loss: 0.0069\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0027 - val_loss: 0.0064\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0030 - val_loss: 0.0064\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0024 - val_loss: 0.0061\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0032 - val_loss: 0.0058\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0027 - val_loss: 0.0062\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0025 - val_loss: 0.0061\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0025 - val_loss: 0.0057\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0026 - val_loss: 0.0061\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0025 - val_loss: 0.0054\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0026 - val_loss: 0.0059\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0024 - val_loss: 0.0054\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0023 - val_loss: 0.0054\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0027 - val_loss: 0.0052\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0025 - val_loss: 0.0049\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0024 - val_loss: 0.0051\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0025 - val_loss: 0.0048\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0019 - val_loss: 0.0046\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0021 - val_loss: 0.0045\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0022 - val_loss: 0.0053\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0019 - val_loss: 0.0045\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0021 - val_loss: 0.0046\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0019 - val_loss: 0.0044\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0021 - val_loss: 0.0046\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0023 - val_loss: 0.0041\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0021 - val_loss: 0.0051\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8cc63b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 161ms/step - loss: 0.0508 - val_loss: 0.0214\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0065 - val_loss: 0.0088\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0043 - val_loss: 0.0086\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0051 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0039 - val_loss: 0.0059\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0036 - val_loss: 0.0056\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0029 - val_loss: 0.0045\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0032 - val_loss: 0.0060\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0022 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0026 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0029 - val_loss: 0.0035\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0022 - val_loss: 0.0033\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0022 - val_loss: 0.0029\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0013 - val_loss: 0.0026\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.8204e-04 - val_loss: 0.0016\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.2552e-04 - val_loss: 0.0020\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.9541e-04 - val_loss: 0.0013\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.1923e-04 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 7.6692e-04 - val_loss: 0.0012\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 9.0716e-04 - val_loss: 0.0012\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 9.5232e-04 - val_loss: 0.0014\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 6.4700e-04 - val_loss: 0.0011\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.1250e-04 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe62a6c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 157ms/step - loss: 0.1624 - val_loss: 0.0116\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0162 - val_loss: 0.0121\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0080 - val_loss: 0.0115\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0062 - val_loss: 0.0113\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0055 - val_loss: 0.0098\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0053 - val_loss: 0.0086\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0051 - val_loss: 0.0067\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0044 - val_loss: 0.0060\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0040 - val_loss: 0.0060\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0039 - val_loss: 0.0057\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0036 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0036 - val_loss: 0.0052\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0031 - val_loss: 0.0043\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0030 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0030 - val_loss: 0.0040\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0028 - val_loss: 0.0032\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0025 - val_loss: 0.0034\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0026 - val_loss: 0.0032\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0024 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0031 - val_loss: 0.0044\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0025 - val_loss: 0.0031\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0023 - val_loss: 0.0030\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0020 - val_loss: 0.0026\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0018 - val_loss: 0.0023\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0018 - val_loss: 0.0022\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0018 - val_loss: 0.0021\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8672a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 8s 139ms/step - loss: 0.0879 - val_loss: 0.0039\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0073 - val_loss: 0.0033\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0030 - val_loss: 0.0032\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0026 - val_loss: 0.0029\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0024 - val_loss: 0.0026\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0017 - val_loss: 0.0020\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 9.9247e-04 - val_loss: 0.0014\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.4063e-04 - val_loss: 0.0013\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 8.9809e-04 - val_loss: 0.0012\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.9521e-04 - val_loss: 0.0011\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 8.5857e-04 - val_loss: 0.0011\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 9.8692e-04 - val_loss: 0.0011\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 9.7605e-04 - val_loss: 0.0011\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 9.6458e-04 - val_loss: 0.0010\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 9.0975e-04 - val_loss: 0.0011\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe62bb170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 137ms/step - loss: 0.0826 - val_loss: 0.0135\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0115 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0093 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0073 - val_loss: 0.0038\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0060 - val_loss: 0.0032\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0057 - val_loss: 0.0034\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0060 - val_loss: 0.0027\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0048 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0045 - val_loss: 0.0025\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0043 - val_loss: 0.0025\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0061 - val_loss: 0.0028\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0049 - val_loss: 0.0025\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0044 - val_loss: 0.0029\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0039 - val_loss: 0.0022\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0024 - val_loss: 0.0030\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0041 - val_loss: 0.0022\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0028 - val_loss: 0.0022\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0034 - val_loss: 0.0023\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0031 - val_loss: 0.0020\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0030 - val_loss: 0.0018\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0025 - val_loss: 0.0015\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0034 - val_loss: 0.0015\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0030 - val_loss: 0.0014\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0028 - val_loss: 0.0011\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0023 - val_loss: 0.0011\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0019 - val_loss: 9.7814e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe4d03d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 135ms/step - loss: 0.0972 - val_loss: 0.0051\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0057 - val_loss: 0.0016\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0026 - val_loss: 0.0015\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0018 - val_loss: 0.0013\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0017 - val_loss: 0.0012\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0017 - val_loss: 0.0012\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0016 - val_loss: 0.0010\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 9.9667e-04\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0015 - val_loss: 0.0010\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0016 - val_loss: 9.2291e-04\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 8.9977e-04\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 9.2517e-04\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 8.5535e-04\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0014 - val_loss: 8.3334e-04\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0012 - val_loss: 7.7008e-04\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 7.1419e-04\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 7.1184e-04\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 6.8522e-04\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0010 - val_loss: 6.4739e-04\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 6.3757e-04\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0010 - val_loss: 6.7996e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 6.1792e-04\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 8.6147e-04 - val_loss: 5.7205e-04\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 9.1483e-04 - val_loss: 5.4840e-04\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.5643e-04 - val_loss: 5.1478e-04\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.3381e-04 - val_loss: 5.0083e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.8711e-04 - val_loss: 4.8268e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.1126e-04 - val_loss: 4.9685e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 9.1131e-04 - val_loss: 4.6998e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.1271e-04 - val_loss: 4.8441e-04\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 7.6398e-04 - val_loss: 4.3715e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.5651e-04 - val_loss: 4.5841e-04\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 8.3274e-04 - val_loss: 4.1532e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.8372e-04 - val_loss: 4.1541e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.3355e-04 - val_loss: 4.4128e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 7.2928e-04 - val_loss: 3.9744e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 5.8436e-04 - val_loss: 3.9005e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 6.7192e-04 - val_loss: 3.7407e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 6.5831e-04 - val_loss: 3.7548e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 6.2751e-04 - val_loss: 3.5655e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 6.1867e-04 - val_loss: 3.5381e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 6.5743e-04 - val_loss: 3.4471e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 6.4075e-04 - val_loss: 3.4132e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 5.5614e-04 - val_loss: 3.3622e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 6.9825e-04 - val_loss: 3.4053e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 6.3183e-04 - val_loss: 3.3973e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe85754d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 148ms/step - loss: 0.0867 - val_loss: 0.0103\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0073 - val_loss: 0.0082\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0043 - val_loss: 0.0069\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0033 - val_loss: 0.0066\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0032 - val_loss: 0.0063\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0030 - val_loss: 0.0060\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0031 - val_loss: 0.0056\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0027 - val_loss: 0.0053\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0027 - val_loss: 0.0049\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0026 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0022 - val_loss: 0.0031\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0023 - val_loss: 0.0029\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0020 - val_loss: 0.0026\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe5f84ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 138ms/step - loss: 0.0594 - val_loss: 0.0178\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0059 - val_loss: 0.0085\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.0027 - val_loss: 0.0082\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0025 - val_loss: 0.0074\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0025 - val_loss: 0.0066\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0021 - val_loss: 0.0061\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0019 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0033\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0013 - val_loss: 0.0027\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0024\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0010 - val_loss: 0.0022\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0010 - val_loss: 0.0023\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 9.8122e-04 - val_loss: 0.0022\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 9.7021e-04 - val_loss: 0.0022\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.3687e-04 - val_loss: 0.0020\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 9.2229e-04 - val_loss: 0.0021\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.7345e-04 - val_loss: 0.0020\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.6689e-04 - val_loss: 0.0021\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.9951e-04 - val_loss: 0.0020\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.3856e-04 - val_loss: 0.0019\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.7423e-04 - val_loss: 0.0019\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.7856e-04 - val_loss: 0.0025\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.9391e-04 - val_loss: 0.0018\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.9094e-04 - val_loss: 0.0018\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.1844e-04 - val_loss: 0.0017\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 8.0795e-04 - val_loss: 0.0018\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.0584e-04 - val_loss: 0.0017\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.9164e-04 - val_loss: 0.0019\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 8.5278e-04 - val_loss: 0.0016\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 7.0868e-04 - val_loss: 0.0016\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 6.7517e-04 - val_loss: 0.0016\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.3715e-04 - val_loss: 0.0015\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.2825e-04 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.8471e-04 - val_loss: 0.0015\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 7.0360e-04 - val_loss: 0.0015\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.6335e-04 - val_loss: 0.0017\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.7544e-04 - val_loss: 0.0015\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8aeb830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 137ms/step - loss: 0.0928 - val_loss: 0.0210\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0065 - val_loss: 0.0077\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0027 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0019 - val_loss: 0.0052\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0020 - val_loss: 0.0051\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0019 - val_loss: 0.0053\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0019 - val_loss: 0.0047\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0021 - val_loss: 0.0047\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0021 - val_loss: 0.0055\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0021 - val_loss: 0.0043\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0016 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 0.0041\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0034\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0034\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0013 - val_loss: 0.0027\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0024\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 9.5752e-04 - val_loss: 0.0022\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.7192e-04 - val_loss: 0.0020\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.0682e-04 - val_loss: 0.0024\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.2200e-04 - val_loss: 0.0017\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 9.0546e-04 - val_loss: 0.0016\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 8.6748e-04 - val_loss: 0.0016\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.1462e-04 - val_loss: 0.0019\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 8.2237e-04 - val_loss: 0.0015\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.9072e-04 - val_loss: 0.0015\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 7.8128e-04 - val_loss: 0.0015\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.4409e-04 - val_loss: 0.0015\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 7.6169e-04 - val_loss: 0.0013\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.8050e-04 - val_loss: 0.0013\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 6.7094e-04 - val_loss: 0.0014\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 5.6713e-04 - val_loss: 0.0015\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 7.0298e-04 - val_loss: 0.0012\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 6.2329e-04 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 6.2162e-04 - val_loss: 0.0014\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 6.1016e-04 - val_loss: 0.0012\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 5.2871e-04 - val_loss: 0.0017\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 6.7232e-04 - val_loss: 0.0011\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe7b0ce60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 139ms/step - loss: 0.1412 - val_loss: 0.0046\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0103 - val_loss: 0.0059\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0034 - val_loss: 0.0044\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0030 - val_loss: 0.0035\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0029 - val_loss: 0.0040\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0026 - val_loss: 0.0033\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.0024 - val_loss: 0.0032\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0025 - val_loss: 0.0029\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0026\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0025 - val_loss: 0.0030\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0018 - val_loss: 0.0023\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0018 - val_loss: 0.0023\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 2s 80ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe8b1acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 7s 155ms/step - loss: 0.1032 - val_loss: 0.0290\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0118 - val_loss: 0.0148\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0060 - val_loss: 0.0108\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0047 - val_loss: 0.0084\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0046 - val_loss: 0.0074\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0041 - val_loss: 0.0067\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0036 - val_loss: 0.0062\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0031 - val_loss: 0.0048\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0031 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0027 - val_loss: 0.0042\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0028 - val_loss: 0.0035\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0024 - val_loss: 0.0034\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0024 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0024 - val_loss: 0.0031\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0022 - val_loss: 0.0030\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0021 - val_loss: 0.0029\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 2s 79ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0018 - val_loss: 0.0022\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 2s 80ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 9.5950e-04 - val_loss: 0.0014\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 9.2441e-04 - val_loss: 0.0014\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe98dfdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 8s 140ms/step - loss: 0.1063 - val_loss: 0.0050\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0089 - val_loss: 0.0026\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0042 - val_loss: 0.0023\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0054 - val_loss: 0.0027\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0034 - val_loss: 0.0020\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0039 - val_loss: 0.0019\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0038 - val_loss: 0.0024\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0038 - val_loss: 0.0018\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0036 - val_loss: 0.0018\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0030 - val_loss: 0.0018\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0052 - val_loss: 0.0014\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0030 - val_loss: 0.0013\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0024 - val_loss: 0.0013\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0034 - val_loss: 0.0012\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0026 - val_loss: 0.0010\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0014 - val_loss: 9.9956e-04\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0018 - val_loss: 0.0012\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0029 - val_loss: 0.0013\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0014 - val_loss: 9.9511e-04\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0020 - val_loss: 9.2018e-04\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0016 - val_loss: 8.9074e-04\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0018 - val_loss: 9.3035e-04\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0015 - val_loss: 0.0010\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0012 - val_loss: 8.4564e-04\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0012 - val_loss: 8.1314e-04\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0017 - val_loss: 8.9348e-04\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.0012 - val_loss: 7.8788e-04\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 8.0899e-04 - val_loss: 7.8672e-04\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 7.1495e-04 - val_loss: 8.2498e-04\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 0.0012 - val_loss: 9.2179e-04\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0012 - val_loss: 7.3141e-04\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 6.8903e-04 - val_loss: 7.3089e-04\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 5.6238e-04 - val_loss: 7.3412e-04\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 6.9981e-04 - val_loss: 7.8275e-04\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 0.0010 - val_loss: 6.9771e-04\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 9.3803e-04 - val_loss: 6.9272e-04\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 1s 79ms/step - loss: 5.6263e-04 - val_loss: 7.5874e-04\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 1s 78ms/step - loss: 9.6654e-04 - val_loss: 9.0610e-04\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fe859fd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG5uqRTl44SQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "46f0b7f7-1f7e-480e-fed9-8056dd171041"
      },
      "source": [
        "fdata # forcasted dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADANIPORTS</th>\n",
              "      <th>ASIANPAINT</th>\n",
              "      <th>AXISBANK</th>\n",
              "      <th>BAJAJ-AUTO</th>\n",
              "      <th>BAJFINANCE</th>\n",
              "      <th>BPCL</th>\n",
              "      <th>BHARTIARTL</th>\n",
              "      <th>BRITANNIA</th>\n",
              "      <th>CIPLA</th>\n",
              "      <th>DIVISLAB</th>\n",
              "      <th>DRREDDY</th>\n",
              "      <th>GAIL</th>\n",
              "      <th>GRASIM</th>\n",
              "      <th>HCLTECH</th>\n",
              "      <th>HDFCBANK</th>\n",
              "      <th>HEROMOTOCO</th>\n",
              "      <th>HINDALCO</th>\n",
              "      <th>HINDUNILVR</th>\n",
              "      <th>HDFC</th>\n",
              "      <th>ICICIBANK</th>\n",
              "      <th>ITC</th>\n",
              "      <th>IOC</th>\n",
              "      <th>INDUSINDBK</th>\n",
              "      <th>INFY</th>\n",
              "      <th>JSWSTEEL</th>\n",
              "      <th>KOTAKBANK</th>\n",
              "      <th>LT</th>\n",
              "      <th>M&amp;M</th>\n",
              "      <th>NTPC</th>\n",
              "      <th>ONGC</th>\n",
              "      <th>POWERGRID</th>\n",
              "      <th>RELIANCE</th>\n",
              "      <th>SBIN</th>\n",
              "      <th>SUNPHARMA</th>\n",
              "      <th>TCS</th>\n",
              "      <th>TATAMOTORS</th>\n",
              "      <th>TATASTEEL</th>\n",
              "      <th>TECHM</th>\n",
              "      <th>TITAN</th>\n",
              "      <th>UPL</th>\n",
              "      <th>ULTRACEMCO</th>\n",
              "      <th>WIPRO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>267.550000</td>\n",
              "      <td>878.750000</td>\n",
              "      <td>449.900000</td>\n",
              "      <td>2518.000000</td>\n",
              "      <td>6095.850000</td>\n",
              "      <td>896.050000</td>\n",
              "      <td>340.500000</td>\n",
              "      <td>2986.400000</td>\n",
              "      <td>655.350000</td>\n",
              "      <td>1162.500000</td>\n",
              "      <td>3108.600000</td>\n",
              "      <td>369.750000</td>\n",
              "      <td>3767.750000</td>\n",
              "      <td>845.850000</td>\n",
              "      <td>1088.750000</td>\n",
              "      <td>2686.050000</td>\n",
              "      <td>84.900000</td>\n",
              "      <td>856.550000</td>\n",
              "      <td>1258.450000</td>\n",
              "      <td>263.000000</td>\n",
              "      <td>327.500000</td>\n",
              "      <td>433.250000</td>\n",
              "      <td>963.850000</td>\n",
              "      <td>1105.250000</td>\n",
              "      <td>1027.000000</td>\n",
              "      <td>727.250000</td>\n",
              "      <td>1289.200000</td>\n",
              "      <td>1265.350000</td>\n",
              "      <td>144.500000</td>\n",
              "      <td>242.500000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>1015.350000</td>\n",
              "      <td>227.800000</td>\n",
              "      <td>815.550000</td>\n",
              "      <td>2416.400000</td>\n",
              "      <td>401.900000</td>\n",
              "      <td>257.400000</td>\n",
              "      <td>520.050000</td>\n",
              "      <td>352.050000</td>\n",
              "      <td>440.350000</td>\n",
              "      <td>2824.000000</td>\n",
              "      <td>556.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>257.950000</td>\n",
              "      <td>880.800000</td>\n",
              "      <td>438.400000</td>\n",
              "      <td>2483.350000</td>\n",
              "      <td>6052.450000</td>\n",
              "      <td>891.450000</td>\n",
              "      <td>326.800000</td>\n",
              "      <td>2972.600000</td>\n",
              "      <td>645.650000</td>\n",
              "      <td>1149.700000</td>\n",
              "      <td>3056.000000</td>\n",
              "      <td>359.800000</td>\n",
              "      <td>3657.250000</td>\n",
              "      <td>845.950000</td>\n",
              "      <td>1070.500000</td>\n",
              "      <td>2637.800000</td>\n",
              "      <td>80.750000</td>\n",
              "      <td>859.000000</td>\n",
              "      <td>1216.700000</td>\n",
              "      <td>255.550000</td>\n",
              "      <td>325.100000</td>\n",
              "      <td>426.450000</td>\n",
              "      <td>934.200000</td>\n",
              "      <td>1078.900000</td>\n",
              "      <td>1044.700000</td>\n",
              "      <td>705.300000</td>\n",
              "      <td>1255.950000</td>\n",
              "      <td>1242.500000</td>\n",
              "      <td>143.300000</td>\n",
              "      <td>238.050000</td>\n",
              "      <td>140.950000</td>\n",
              "      <td>995.300000</td>\n",
              "      <td>220.700000</td>\n",
              "      <td>799.100000</td>\n",
              "      <td>2369.600000</td>\n",
              "      <td>377.050000</td>\n",
              "      <td>256.900000</td>\n",
              "      <td>516.400000</td>\n",
              "      <td>351.400000</td>\n",
              "      <td>433.850000</td>\n",
              "      <td>2749.150000</td>\n",
              "      <td>557.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>255.700000</td>\n",
              "      <td>900.650000</td>\n",
              "      <td>436.450000</td>\n",
              "      <td>2502.350000</td>\n",
              "      <td>5997.850000</td>\n",
              "      <td>902.650000</td>\n",
              "      <td>323.450000</td>\n",
              "      <td>2966.700000</td>\n",
              "      <td>640.850000</td>\n",
              "      <td>1147.500000</td>\n",
              "      <td>3070.650000</td>\n",
              "      <td>372.250000</td>\n",
              "      <td>3659.100000</td>\n",
              "      <td>842.800000</td>\n",
              "      <td>1062.400000</td>\n",
              "      <td>2616.850000</td>\n",
              "      <td>82.600000</td>\n",
              "      <td>847.950000</td>\n",
              "      <td>1209.400000</td>\n",
              "      <td>256.700000</td>\n",
              "      <td>324.850000</td>\n",
              "      <td>442.650000</td>\n",
              "      <td>935.250000</td>\n",
              "      <td>1074.050000</td>\n",
              "      <td>1083.100000</td>\n",
              "      <td>707.750000</td>\n",
              "      <td>1256.900000</td>\n",
              "      <td>1249.200000</td>\n",
              "      <td>142.250000</td>\n",
              "      <td>241.850000</td>\n",
              "      <td>139.150000</td>\n",
              "      <td>1005.150000</td>\n",
              "      <td>217.750000</td>\n",
              "      <td>800.500000</td>\n",
              "      <td>2348.950000</td>\n",
              "      <td>374.450000</td>\n",
              "      <td>274.300000</td>\n",
              "      <td>521.900000</td>\n",
              "      <td>347.700000</td>\n",
              "      <td>435.300000</td>\n",
              "      <td>2733.050000</td>\n",
              "      <td>556.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>251.800000</td>\n",
              "      <td>886.400000</td>\n",
              "      <td>430.700000</td>\n",
              "      <td>2485.500000</td>\n",
              "      <td>6107.250000</td>\n",
              "      <td>912.300000</td>\n",
              "      <td>322.200000</td>\n",
              "      <td>3001.700000</td>\n",
              "      <td>652.100000</td>\n",
              "      <td>1137.200000</td>\n",
              "      <td>3046.250000</td>\n",
              "      <td>372.700000</td>\n",
              "      <td>3652.400000</td>\n",
              "      <td>841.400000</td>\n",
              "      <td>1067.100000</td>\n",
              "      <td>2578.200000</td>\n",
              "      <td>80.550000</td>\n",
              "      <td>843.050000</td>\n",
              "      <td>1209.300000</td>\n",
              "      <td>250.100000</td>\n",
              "      <td>315.100000</td>\n",
              "      <td>450.850000</td>\n",
              "      <td>943.950000</td>\n",
              "      <td>1069.350000</td>\n",
              "      <td>1058.200000</td>\n",
              "      <td>703.600000</td>\n",
              "      <td>1236.850000</td>\n",
              "      <td>1226.150000</td>\n",
              "      <td>141.450000</td>\n",
              "      <td>237.550000</td>\n",
              "      <td>139.500000</td>\n",
              "      <td>1032.200000</td>\n",
              "      <td>216.850000</td>\n",
              "      <td>791.950000</td>\n",
              "      <td>2381.600000</td>\n",
              "      <td>365.900000</td>\n",
              "      <td>268.750000</td>\n",
              "      <td>525.100000</td>\n",
              "      <td>347.850000</td>\n",
              "      <td>443.350000</td>\n",
              "      <td>2735.300000</td>\n",
              "      <td>555.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>245.000000</td>\n",
              "      <td>872.350000</td>\n",
              "      <td>409.250000</td>\n",
              "      <td>2419.250000</td>\n",
              "      <td>6106.300000</td>\n",
              "      <td>884.250000</td>\n",
              "      <td>322.350000</td>\n",
              "      <td>2946.550000</td>\n",
              "      <td>637.450000</td>\n",
              "      <td>1126.950000</td>\n",
              "      <td>2991.800000</td>\n",
              "      <td>370.600000</td>\n",
              "      <td>3557.200000</td>\n",
              "      <td>825.050000</td>\n",
              "      <td>1056.200000</td>\n",
              "      <td>2520.100000</td>\n",
              "      <td>76.650000</td>\n",
              "      <td>820.250000</td>\n",
              "      <td>1179.450000</td>\n",
              "      <td>246.750000</td>\n",
              "      <td>309.600000</td>\n",
              "      <td>441.550000</td>\n",
              "      <td>925.500000</td>\n",
              "      <td>1050.800000</td>\n",
              "      <td>1026.350000</td>\n",
              "      <td>691.650000</td>\n",
              "      <td>1206.400000</td>\n",
              "      <td>1197.450000</td>\n",
              "      <td>137.850000</td>\n",
              "      <td>226.600000</td>\n",
              "      <td>138.100000</td>\n",
              "      <td>1013.250000</td>\n",
              "      <td>209.550000</td>\n",
              "      <td>783.800000</td>\n",
              "      <td>2371.250000</td>\n",
              "      <td>343.550000</td>\n",
              "      <td>249.900000</td>\n",
              "      <td>510.800000</td>\n",
              "      <td>344.050000</td>\n",
              "      <td>417.400000</td>\n",
              "      <td>2667.700000</td>\n",
              "      <td>549.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>491.274694</td>\n",
              "      <td>2928.861454</td>\n",
              "      <td>633.258949</td>\n",
              "      <td>3453.281130</td>\n",
              "      <td>5261.577293</td>\n",
              "      <td>376.306029</td>\n",
              "      <td>517.094406</td>\n",
              "      <td>3548.386239</td>\n",
              "      <td>839.259267</td>\n",
              "      <td>3741.909291</td>\n",
              "      <td>5184.407090</td>\n",
              "      <td>122.455771</td>\n",
              "      <td>918.113701</td>\n",
              "      <td>956.527491</td>\n",
              "      <td>1421.980039</td>\n",
              "      <td>3073.712970</td>\n",
              "      <td>239.984211</td>\n",
              "      <td>2328.942953</td>\n",
              "      <td>2562.821015</td>\n",
              "      <td>542.579025</td>\n",
              "      <td>208.451654</td>\n",
              "      <td>90.393951</td>\n",
              "      <td>875.289496</td>\n",
              "      <td>1233.586867</td>\n",
              "      <td>387.521254</td>\n",
              "      <td>1969.456457</td>\n",
              "      <td>1324.225139</td>\n",
              "      <td>732.642668</td>\n",
              "      <td>98.829868</td>\n",
              "      <td>92.660906</td>\n",
              "      <td>189.619047</td>\n",
              "      <td>2008.620153</td>\n",
              "      <td>283.332065</td>\n",
              "      <td>588.484074</td>\n",
              "      <td>2964.145698</td>\n",
              "      <td>182.208902</td>\n",
              "      <td>636.997120</td>\n",
              "      <td>977.273507</td>\n",
              "      <td>1564.678269</td>\n",
              "      <td>464.228954</td>\n",
              "      <td>5412.951857</td>\n",
              "      <td>405.884540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>494.365585</td>\n",
              "      <td>2950.587272</td>\n",
              "      <td>642.454811</td>\n",
              "      <td>3460.531913</td>\n",
              "      <td>5214.271489</td>\n",
              "      <td>378.794869</td>\n",
              "      <td>517.024177</td>\n",
              "      <td>3541.970118</td>\n",
              "      <td>840.530423</td>\n",
              "      <td>3763.292511</td>\n",
              "      <td>5211.008466</td>\n",
              "      <td>123.581692</td>\n",
              "      <td>926.237696</td>\n",
              "      <td>970.705303</td>\n",
              "      <td>1424.306567</td>\n",
              "      <td>3062.645048</td>\n",
              "      <td>243.680047</td>\n",
              "      <td>2343.034020</td>\n",
              "      <td>2591.699922</td>\n",
              "      <td>542.996305</td>\n",
              "      <td>209.260195</td>\n",
              "      <td>90.759617</td>\n",
              "      <td>884.620764</td>\n",
              "      <td>1249.513140</td>\n",
              "      <td>391.210068</td>\n",
              "      <td>1954.203619</td>\n",
              "      <td>1330.442264</td>\n",
              "      <td>738.977534</td>\n",
              "      <td>98.655735</td>\n",
              "      <td>92.840294</td>\n",
              "      <td>189.297551</td>\n",
              "      <td>1997.499336</td>\n",
              "      <td>284.503609</td>\n",
              "      <td>591.692702</td>\n",
              "      <td>3076.016313</td>\n",
              "      <td>185.239331</td>\n",
              "      <td>649.198232</td>\n",
              "      <td>986.321778</td>\n",
              "      <td>1568.277384</td>\n",
              "      <td>467.710173</td>\n",
              "      <td>5423.690146</td>\n",
              "      <td>411.751053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>494.077483</td>\n",
              "      <td>2968.564990</td>\n",
              "      <td>652.714491</td>\n",
              "      <td>3450.429113</td>\n",
              "      <td>5131.973948</td>\n",
              "      <td>380.679815</td>\n",
              "      <td>520.956161</td>\n",
              "      <td>3534.772118</td>\n",
              "      <td>840.577841</td>\n",
              "      <td>3780.892833</td>\n",
              "      <td>5230.635524</td>\n",
              "      <td>125.142222</td>\n",
              "      <td>935.802908</td>\n",
              "      <td>980.246192</td>\n",
              "      <td>1424.276144</td>\n",
              "      <td>3054.779655</td>\n",
              "      <td>248.340742</td>\n",
              "      <td>2354.917462</td>\n",
              "      <td>2620.511941</td>\n",
              "      <td>545.546724</td>\n",
              "      <td>209.169689</td>\n",
              "      <td>91.215649</td>\n",
              "      <td>894.055990</td>\n",
              "      <td>1253.952873</td>\n",
              "      <td>394.682537</td>\n",
              "      <td>1943.300011</td>\n",
              "      <td>1335.702657</td>\n",
              "      <td>744.206159</td>\n",
              "      <td>98.492407</td>\n",
              "      <td>93.214339</td>\n",
              "      <td>189.951123</td>\n",
              "      <td>1960.261939</td>\n",
              "      <td>286.657649</td>\n",
              "      <td>594.324964</td>\n",
              "      <td>3123.973326</td>\n",
              "      <td>188.323739</td>\n",
              "      <td>661.642953</td>\n",
              "      <td>993.366916</td>\n",
              "      <td>1571.295612</td>\n",
              "      <td>470.833660</td>\n",
              "      <td>5477.971253</td>\n",
              "      <td>419.019559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>498.440656</td>\n",
              "      <td>2980.840729</td>\n",
              "      <td>664.700296</td>\n",
              "      <td>3427.835423</td>\n",
              "      <td>5064.623183</td>\n",
              "      <td>382.224315</td>\n",
              "      <td>532.507496</td>\n",
              "      <td>3530.811061</td>\n",
              "      <td>840.703284</td>\n",
              "      <td>3785.958295</td>\n",
              "      <td>5236.785047</td>\n",
              "      <td>126.931174</td>\n",
              "      <td>946.356803</td>\n",
              "      <td>979.210136</td>\n",
              "      <td>1422.232382</td>\n",
              "      <td>3045.045785</td>\n",
              "      <td>254.828551</td>\n",
              "      <td>2359.228677</td>\n",
              "      <td>2647.954364</td>\n",
              "      <td>547.833435</td>\n",
              "      <td>208.091002</td>\n",
              "      <td>91.726080</td>\n",
              "      <td>904.953562</td>\n",
              "      <td>1241.553002</td>\n",
              "      <td>398.098431</td>\n",
              "      <td>1933.316631</td>\n",
              "      <td>1345.246375</td>\n",
              "      <td>748.593335</td>\n",
              "      <td>98.305861</td>\n",
              "      <td>93.761497</td>\n",
              "      <td>191.384454</td>\n",
              "      <td>1944.288859</td>\n",
              "      <td>289.514705</td>\n",
              "      <td>595.288120</td>\n",
              "      <td>3101.244200</td>\n",
              "      <td>191.309821</td>\n",
              "      <td>676.269430</td>\n",
              "      <td>997.578717</td>\n",
              "      <td>1566.866699</td>\n",
              "      <td>474.038545</td>\n",
              "      <td>5519.515558</td>\n",
              "      <td>424.763773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>505.389286</td>\n",
              "      <td>2995.312363</td>\n",
              "      <td>674.989525</td>\n",
              "      <td>3437.365734</td>\n",
              "      <td>5026.550553</td>\n",
              "      <td>384.201742</td>\n",
              "      <td>540.518163</td>\n",
              "      <td>3534.013089</td>\n",
              "      <td>844.471949</td>\n",
              "      <td>3784.405425</td>\n",
              "      <td>5253.940450</td>\n",
              "      <td>128.661574</td>\n",
              "      <td>957.411610</td>\n",
              "      <td>979.580855</td>\n",
              "      <td>1420.784164</td>\n",
              "      <td>3056.652743</td>\n",
              "      <td>261.179883</td>\n",
              "      <td>2357.579056</td>\n",
              "      <td>2670.615929</td>\n",
              "      <td>549.057300</td>\n",
              "      <td>206.312116</td>\n",
              "      <td>92.328301</td>\n",
              "      <td>915.625309</td>\n",
              "      <td>1246.740113</td>\n",
              "      <td>400.985728</td>\n",
              "      <td>1930.751417</td>\n",
              "      <td>1363.432592</td>\n",
              "      <td>754.416457</td>\n",
              "      <td>98.232350</td>\n",
              "      <td>94.536574</td>\n",
              "      <td>193.983672</td>\n",
              "      <td>1961.072812</td>\n",
              "      <td>291.030013</td>\n",
              "      <td>598.521291</td>\n",
              "      <td>3106.353592</td>\n",
              "      <td>194.074256</td>\n",
              "      <td>690.634451</td>\n",
              "      <td>1006.248273</td>\n",
              "      <td>1559.550293</td>\n",
              "      <td>478.563110</td>\n",
              "      <td>5600.989268</td>\n",
              "      <td>434.237711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1245 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ADANIPORTS   ASIANPAINT    AXISBANK  ...         UPL   ULTRACEMCO       WIPRO\n",
              "0     267.550000   878.750000  449.900000  ...  440.350000  2824.000000  556.450000\n",
              "1     257.950000   880.800000  438.400000  ...  433.850000  2749.150000  557.700000\n",
              "2     255.700000   900.650000  436.450000  ...  435.300000  2733.050000  556.900000\n",
              "3     251.800000   886.400000  430.700000  ...  443.350000  2735.300000  555.100000\n",
              "4     245.000000   872.350000  409.250000  ...  417.400000  2667.700000  549.850000\n",
              "...          ...          ...         ...  ...         ...          ...         ...\n",
              "1240  491.274694  2928.861454  633.258949  ...  464.228954  5412.951857  405.884540\n",
              "1241  494.365585  2950.587272  642.454811  ...  467.710173  5423.690146  411.751053\n",
              "1242  494.077483  2968.564990  652.714491  ...  470.833660  5477.971253  419.019559\n",
              "1243  498.440656  2980.840729  664.700296  ...  474.038545  5519.515558  424.763773\n",
              "1244  505.389286  2995.312363  674.989525  ...  478.563110  5600.989268  434.237711\n",
              "\n",
              "[1245 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "HUIDhvD2LiOI",
        "outputId": "d0dbc8e8-d141-49f1-e969-af35f2d1c1b7"
      },
      "source": [
        "alldata # actual dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADANIPORTS</th>\n",
              "      <th>ASIANPAINT</th>\n",
              "      <th>AXISBANK</th>\n",
              "      <th>BAJAJ-AUTO</th>\n",
              "      <th>BAJFINANCE</th>\n",
              "      <th>BPCL</th>\n",
              "      <th>BHARTIARTL</th>\n",
              "      <th>BRITANNIA</th>\n",
              "      <th>CIPLA</th>\n",
              "      <th>DIVISLAB</th>\n",
              "      <th>DRREDDY</th>\n",
              "      <th>GAIL</th>\n",
              "      <th>GRASIM</th>\n",
              "      <th>HCLTECH</th>\n",
              "      <th>HDFCBANK</th>\n",
              "      <th>HEROMOTOCO</th>\n",
              "      <th>HINDALCO</th>\n",
              "      <th>HINDUNILVR</th>\n",
              "      <th>HDFC</th>\n",
              "      <th>ICICIBANK</th>\n",
              "      <th>ITC</th>\n",
              "      <th>IOC</th>\n",
              "      <th>INDUSINDBK</th>\n",
              "      <th>INFY</th>\n",
              "      <th>JSWSTEEL</th>\n",
              "      <th>KOTAKBANK</th>\n",
              "      <th>LT</th>\n",
              "      <th>M&amp;M</th>\n",
              "      <th>NTPC</th>\n",
              "      <th>ONGC</th>\n",
              "      <th>POWERGRID</th>\n",
              "      <th>RELIANCE</th>\n",
              "      <th>SBIN</th>\n",
              "      <th>SUNPHARMA</th>\n",
              "      <th>TCS</th>\n",
              "      <th>TATAMOTORS</th>\n",
              "      <th>TATASTEEL</th>\n",
              "      <th>TECHM</th>\n",
              "      <th>TITAN</th>\n",
              "      <th>UPL</th>\n",
              "      <th>ULTRACEMCO</th>\n",
              "      <th>WIPRO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>267.55</td>\n",
              "      <td>878.75</td>\n",
              "      <td>449.90</td>\n",
              "      <td>2518.00</td>\n",
              "      <td>6095.85</td>\n",
              "      <td>896.05</td>\n",
              "      <td>340.50</td>\n",
              "      <td>2986.40</td>\n",
              "      <td>655.35</td>\n",
              "      <td>1162.50</td>\n",
              "      <td>3108.60</td>\n",
              "      <td>369.75</td>\n",
              "      <td>3767.75</td>\n",
              "      <td>845.85</td>\n",
              "      <td>1088.75</td>\n",
              "      <td>2686.05</td>\n",
              "      <td>84.90</td>\n",
              "      <td>856.55</td>\n",
              "      <td>1258.45</td>\n",
              "      <td>263.00</td>\n",
              "      <td>327.50</td>\n",
              "      <td>433.25</td>\n",
              "      <td>963.85</td>\n",
              "      <td>1105.25</td>\n",
              "      <td>1027.00</td>\n",
              "      <td>727.25</td>\n",
              "      <td>1289.20</td>\n",
              "      <td>1265.35</td>\n",
              "      <td>144.50</td>\n",
              "      <td>242.50</td>\n",
              "      <td>141.00</td>\n",
              "      <td>1015.35</td>\n",
              "      <td>227.80</td>\n",
              "      <td>815.55</td>\n",
              "      <td>2416.40</td>\n",
              "      <td>401.90</td>\n",
              "      <td>257.40</td>\n",
              "      <td>520.05</td>\n",
              "      <td>352.05</td>\n",
              "      <td>440.35</td>\n",
              "      <td>2824.00</td>\n",
              "      <td>556.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>257.95</td>\n",
              "      <td>880.80</td>\n",
              "      <td>438.40</td>\n",
              "      <td>2483.35</td>\n",
              "      <td>6052.45</td>\n",
              "      <td>891.45</td>\n",
              "      <td>326.80</td>\n",
              "      <td>2972.60</td>\n",
              "      <td>645.65</td>\n",
              "      <td>1149.70</td>\n",
              "      <td>3056.00</td>\n",
              "      <td>359.80</td>\n",
              "      <td>3657.25</td>\n",
              "      <td>845.95</td>\n",
              "      <td>1070.50</td>\n",
              "      <td>2637.80</td>\n",
              "      <td>80.75</td>\n",
              "      <td>859.00</td>\n",
              "      <td>1216.70</td>\n",
              "      <td>255.55</td>\n",
              "      <td>325.10</td>\n",
              "      <td>426.45</td>\n",
              "      <td>934.20</td>\n",
              "      <td>1078.90</td>\n",
              "      <td>1044.70</td>\n",
              "      <td>705.30</td>\n",
              "      <td>1255.95</td>\n",
              "      <td>1242.50</td>\n",
              "      <td>143.30</td>\n",
              "      <td>238.05</td>\n",
              "      <td>140.95</td>\n",
              "      <td>995.30</td>\n",
              "      <td>220.70</td>\n",
              "      <td>799.10</td>\n",
              "      <td>2369.60</td>\n",
              "      <td>377.05</td>\n",
              "      <td>256.90</td>\n",
              "      <td>516.40</td>\n",
              "      <td>351.40</td>\n",
              "      <td>433.85</td>\n",
              "      <td>2749.15</td>\n",
              "      <td>557.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>255.70</td>\n",
              "      <td>900.65</td>\n",
              "      <td>436.45</td>\n",
              "      <td>2502.35</td>\n",
              "      <td>5997.85</td>\n",
              "      <td>902.65</td>\n",
              "      <td>323.45</td>\n",
              "      <td>2966.70</td>\n",
              "      <td>640.85</td>\n",
              "      <td>1147.50</td>\n",
              "      <td>3070.65</td>\n",
              "      <td>372.25</td>\n",
              "      <td>3659.10</td>\n",
              "      <td>842.80</td>\n",
              "      <td>1062.40</td>\n",
              "      <td>2616.85</td>\n",
              "      <td>82.60</td>\n",
              "      <td>847.95</td>\n",
              "      <td>1209.40</td>\n",
              "      <td>256.70</td>\n",
              "      <td>324.85</td>\n",
              "      <td>442.65</td>\n",
              "      <td>935.25</td>\n",
              "      <td>1074.05</td>\n",
              "      <td>1083.10</td>\n",
              "      <td>707.75</td>\n",
              "      <td>1256.90</td>\n",
              "      <td>1249.20</td>\n",
              "      <td>142.25</td>\n",
              "      <td>241.85</td>\n",
              "      <td>139.15</td>\n",
              "      <td>1005.15</td>\n",
              "      <td>217.75</td>\n",
              "      <td>800.50</td>\n",
              "      <td>2348.95</td>\n",
              "      <td>374.45</td>\n",
              "      <td>274.30</td>\n",
              "      <td>521.90</td>\n",
              "      <td>347.70</td>\n",
              "      <td>435.30</td>\n",
              "      <td>2733.05</td>\n",
              "      <td>556.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>251.80</td>\n",
              "      <td>886.40</td>\n",
              "      <td>430.70</td>\n",
              "      <td>2485.50</td>\n",
              "      <td>6107.25</td>\n",
              "      <td>912.30</td>\n",
              "      <td>322.20</td>\n",
              "      <td>3001.70</td>\n",
              "      <td>652.10</td>\n",
              "      <td>1137.20</td>\n",
              "      <td>3046.25</td>\n",
              "      <td>372.70</td>\n",
              "      <td>3652.40</td>\n",
              "      <td>841.40</td>\n",
              "      <td>1067.10</td>\n",
              "      <td>2578.20</td>\n",
              "      <td>80.55</td>\n",
              "      <td>843.05</td>\n",
              "      <td>1209.30</td>\n",
              "      <td>250.10</td>\n",
              "      <td>315.10</td>\n",
              "      <td>450.85</td>\n",
              "      <td>943.95</td>\n",
              "      <td>1069.35</td>\n",
              "      <td>1058.20</td>\n",
              "      <td>703.60</td>\n",
              "      <td>1236.85</td>\n",
              "      <td>1226.15</td>\n",
              "      <td>141.45</td>\n",
              "      <td>237.55</td>\n",
              "      <td>139.50</td>\n",
              "      <td>1032.20</td>\n",
              "      <td>216.85</td>\n",
              "      <td>791.95</td>\n",
              "      <td>2381.60</td>\n",
              "      <td>365.90</td>\n",
              "      <td>268.75</td>\n",
              "      <td>525.10</td>\n",
              "      <td>347.85</td>\n",
              "      <td>443.35</td>\n",
              "      <td>2735.30</td>\n",
              "      <td>555.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>245.00</td>\n",
              "      <td>872.35</td>\n",
              "      <td>409.25</td>\n",
              "      <td>2419.25</td>\n",
              "      <td>6106.30</td>\n",
              "      <td>884.25</td>\n",
              "      <td>322.35</td>\n",
              "      <td>2946.55</td>\n",
              "      <td>637.45</td>\n",
              "      <td>1126.95</td>\n",
              "      <td>2991.80</td>\n",
              "      <td>370.60</td>\n",
              "      <td>3557.20</td>\n",
              "      <td>825.05</td>\n",
              "      <td>1056.20</td>\n",
              "      <td>2520.10</td>\n",
              "      <td>76.65</td>\n",
              "      <td>820.25</td>\n",
              "      <td>1179.45</td>\n",
              "      <td>246.75</td>\n",
              "      <td>309.60</td>\n",
              "      <td>441.55</td>\n",
              "      <td>925.50</td>\n",
              "      <td>1050.80</td>\n",
              "      <td>1026.35</td>\n",
              "      <td>691.65</td>\n",
              "      <td>1206.40</td>\n",
              "      <td>1197.45</td>\n",
              "      <td>137.85</td>\n",
              "      <td>226.60</td>\n",
              "      <td>138.10</td>\n",
              "      <td>1013.25</td>\n",
              "      <td>209.55</td>\n",
              "      <td>783.80</td>\n",
              "      <td>2371.25</td>\n",
              "      <td>343.55</td>\n",
              "      <td>249.90</td>\n",
              "      <td>510.80</td>\n",
              "      <td>344.05</td>\n",
              "      <td>417.40</td>\n",
              "      <td>2667.70</td>\n",
              "      <td>549.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>499.45</td>\n",
              "      <td>2793.85</td>\n",
              "      <td>664.45</td>\n",
              "      <td>3492.65</td>\n",
              "      <td>5119.00</td>\n",
              "      <td>392.00</td>\n",
              "      <td>514.00</td>\n",
              "      <td>3551.10</td>\n",
              "      <td>827.25</td>\n",
              "      <td>3842.10</td>\n",
              "      <td>5286.90</td>\n",
              "      <td>129.30</td>\n",
              "      <td>961.30</td>\n",
              "      <td>991.35</td>\n",
              "      <td>1426.70</td>\n",
              "      <td>3067.20</td>\n",
              "      <td>250.30</td>\n",
              "      <td>2450.55</td>\n",
              "      <td>2651.85</td>\n",
              "      <td>537.25</td>\n",
              "      <td>211.45</td>\n",
              "      <td>93.00</td>\n",
              "      <td>921.65</td>\n",
              "      <td>1293.80</td>\n",
              "      <td>395.25</td>\n",
              "      <td>1959.75</td>\n",
              "      <td>1306.30</td>\n",
              "      <td>740.10</td>\n",
              "      <td>97.60</td>\n",
              "      <td>94.95</td>\n",
              "      <td>188.00</td>\n",
              "      <td>1966.10</td>\n",
              "      <td>281.75</td>\n",
              "      <td>603.45</td>\n",
              "      <td>3093.00</td>\n",
              "      <td>193.20</td>\n",
              "      <td>680.55</td>\n",
              "      <td>1003.85</td>\n",
              "      <td>1570.95</td>\n",
              "      <td>471.25</td>\n",
              "      <td>5341.20</td>\n",
              "      <td>406.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>496.80</td>\n",
              "      <td>2805.35</td>\n",
              "      <td>654.25</td>\n",
              "      <td>3462.70</td>\n",
              "      <td>5030.30</td>\n",
              "      <td>387.30</td>\n",
              "      <td>525.30</td>\n",
              "      <td>3539.70</td>\n",
              "      <td>824.80</td>\n",
              "      <td>3879.85</td>\n",
              "      <td>5288.30</td>\n",
              "      <td>134.00</td>\n",
              "      <td>984.60</td>\n",
              "      <td>978.20</td>\n",
              "      <td>1420.55</td>\n",
              "      <td>3083.55</td>\n",
              "      <td>259.05</td>\n",
              "      <td>2417.30</td>\n",
              "      <td>2638.85</td>\n",
              "      <td>546.70</td>\n",
              "      <td>205.40</td>\n",
              "      <td>94.10</td>\n",
              "      <td>922.35</td>\n",
              "      <td>1282.10</td>\n",
              "      <td>401.70</td>\n",
              "      <td>1970.40</td>\n",
              "      <td>1314.00</td>\n",
              "      <td>736.10</td>\n",
              "      <td>97.85</td>\n",
              "      <td>96.95</td>\n",
              "      <td>196.15</td>\n",
              "      <td>1914.25</td>\n",
              "      <td>285.05</td>\n",
              "      <td>605.30</td>\n",
              "      <td>3051.50</td>\n",
              "      <td>195.40</td>\n",
              "      <td>683.80</td>\n",
              "      <td>997.15</td>\n",
              "      <td>1572.60</td>\n",
              "      <td>472.40</td>\n",
              "      <td>5448.35</td>\n",
              "      <td>406.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>513.85</td>\n",
              "      <td>2792.25</td>\n",
              "      <td>671.10</td>\n",
              "      <td>3437.95</td>\n",
              "      <td>5081.00</td>\n",
              "      <td>392.20</td>\n",
              "      <td>545.25</td>\n",
              "      <td>3552.80</td>\n",
              "      <td>826.55</td>\n",
              "      <td>3803.05</td>\n",
              "      <td>5270.90</td>\n",
              "      <td>133.45</td>\n",
              "      <td>993.85</td>\n",
              "      <td>962.55</td>\n",
              "      <td>1416.25</td>\n",
              "      <td>3055.25</td>\n",
              "      <td>272.90</td>\n",
              "      <td>2368.85</td>\n",
              "      <td>2661.35</td>\n",
              "      <td>541.10</td>\n",
              "      <td>202.80</td>\n",
              "      <td>94.50</td>\n",
              "      <td>952.05</td>\n",
              "      <td>1262.15</td>\n",
              "      <td>405.40</td>\n",
              "      <td>1952.40</td>\n",
              "      <td>1338.95</td>\n",
              "      <td>744.40</td>\n",
              "      <td>97.00</td>\n",
              "      <td>97.90</td>\n",
              "      <td>197.05</td>\n",
              "      <td>1911.15</td>\n",
              "      <td>287.70</td>\n",
              "      <td>601.90</td>\n",
              "      <td>3032.80</td>\n",
              "      <td>196.75</td>\n",
              "      <td>722.80</td>\n",
              "      <td>994.75</td>\n",
              "      <td>1542.35</td>\n",
              "      <td>482.50</td>\n",
              "      <td>5397.95</td>\n",
              "      <td>406.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>517.00</td>\n",
              "      <td>2844.70</td>\n",
              "      <td>672.70</td>\n",
              "      <td>3529.15</td>\n",
              "      <td>5082.00</td>\n",
              "      <td>400.15</td>\n",
              "      <td>540.25</td>\n",
              "      <td>3575.25</td>\n",
              "      <td>838.70</td>\n",
              "      <td>3859.15</td>\n",
              "      <td>5338.25</td>\n",
              "      <td>132.75</td>\n",
              "      <td>1004.30</td>\n",
              "      <td>994.65</td>\n",
              "      <td>1431.65</td>\n",
              "      <td>3161.10</td>\n",
              "      <td>268.20</td>\n",
              "      <td>2391.20</td>\n",
              "      <td>2657.50</td>\n",
              "      <td>542.05</td>\n",
              "      <td>201.50</td>\n",
              "      <td>96.15</td>\n",
              "      <td>939.80</td>\n",
              "      <td>1312.10</td>\n",
              "      <td>402.85</td>\n",
              "      <td>1970.70</td>\n",
              "      <td>1373.40</td>\n",
              "      <td>770.50</td>\n",
              "      <td>100.15</td>\n",
              "      <td>100.65</td>\n",
              "      <td>203.55</td>\n",
              "      <td>1933.70</td>\n",
              "      <td>286.00</td>\n",
              "      <td>620.80</td>\n",
              "      <td>3120.90</td>\n",
              "      <td>198.15</td>\n",
              "      <td>713.15</td>\n",
              "      <td>1051.10</td>\n",
              "      <td>1548.60</td>\n",
              "      <td>503.65</td>\n",
              "      <td>5591.75</td>\n",
              "      <td>430.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>508.35</td>\n",
              "      <td>2849.30</td>\n",
              "      <td>666.95</td>\n",
              "      <td>3617.35</td>\n",
              "      <td>4984.25</td>\n",
              "      <td>401.75</td>\n",
              "      <td>547.15</td>\n",
              "      <td>3612.85</td>\n",
              "      <td>856.85</td>\n",
              "      <td>3822.40</td>\n",
              "      <td>5416.80</td>\n",
              "      <td>135.70</td>\n",
              "      <td>1004.30</td>\n",
              "      <td>1055.10</td>\n",
              "      <td>1451.45</td>\n",
              "      <td>3197.70</td>\n",
              "      <td>264.70</td>\n",
              "      <td>2429.10</td>\n",
              "      <td>2751.20</td>\n",
              "      <td>544.70</td>\n",
              "      <td>202.50</td>\n",
              "      <td>96.00</td>\n",
              "      <td>929.10</td>\n",
              "      <td>1376.20</td>\n",
              "      <td>399.05</td>\n",
              "      <td>1938.15</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>789.10</td>\n",
              "      <td>99.00</td>\n",
              "      <td>102.55</td>\n",
              "      <td>203.55</td>\n",
              "      <td>1897.25</td>\n",
              "      <td>282.50</td>\n",
              "      <td>621.00</td>\n",
              "      <td>3176.45</td>\n",
              "      <td>220.65</td>\n",
              "      <td>695.65</td>\n",
              "      <td>1077.60</td>\n",
              "      <td>1563.90</td>\n",
              "      <td>497.45</td>\n",
              "      <td>5623.70</td>\n",
              "      <td>446.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1245 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ADANIPORTS  ASIANPAINT  AXISBANK  ...     UPL  ULTRACEMCO   WIPRO\n",
              "0         267.55      878.75    449.90  ...  440.35     2824.00  556.45\n",
              "1         257.95      880.80    438.40  ...  433.85     2749.15  557.70\n",
              "2         255.70      900.65    436.45  ...  435.30     2733.05  556.90\n",
              "3         251.80      886.40    430.70  ...  443.35     2735.30  555.10\n",
              "4         245.00      872.35    409.25  ...  417.40     2667.70  549.85\n",
              "...          ...         ...       ...  ...     ...         ...     ...\n",
              "1240      499.45     2793.85    664.45  ...  471.25     5341.20  406.30\n",
              "1241      496.80     2805.35    654.25  ...  472.40     5448.35  406.40\n",
              "1242      513.85     2792.25    671.10  ...  482.50     5397.95  406.75\n",
              "1243      517.00     2844.70    672.70  ...  503.65     5591.75  430.20\n",
              "1244      508.35     2849.30    666.95  ...  497.45     5623.70  446.80\n",
              "\n",
              "[1245 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNLiv9PUL2er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2519e5-9815-494d-8575-afb797438efc"
      },
      "source": [
        "rmse                                  #between original data(after 31-12-2020) and dynamically predicted data (after 31-12-2020)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.042989807410568785,\n",
              " 0.0702256782142951,\n",
              " 0.030288418632676003,\n",
              " 0.05233953442362253,\n",
              " 0.0089872430052715,\n",
              " 0.017278831331289653,\n",
              " 0.033818823265396425,\n",
              " 0.007362592398801193,\n",
              " 0.024016429131970593,\n",
              " 0.031593480520855616,\n",
              " 0.029711734138390226,\n",
              " 0.015646008431992924,\n",
              " 0.010330101415843793,\n",
              " 0.045437015023994616,\n",
              " 0.008186619164917922,\n",
              " 0.02888507233075661,\n",
              " 0.06563340355807318,\n",
              " 0.046986049277614635,\n",
              " 0.032879400107791265,\n",
              " 0.017760179183962214,\n",
              " 0.024913043723666182,\n",
              " 0.005754544171136107,\n",
              " 0.02308043386781796,\n",
              " 0.07512688942347971,\n",
              " 0.006183528222363233,\n",
              " 0.013152633166523172,\n",
              " 0.01449810948781094,\n",
              " 0.01420521024036168,\n",
              " 0.009996273825086678,\n",
              " 0.019788315876967184,\n",
              " 0.07281704546595948,\n",
              " 0.030866729423332014,\n",
              " 0.017163819575954368,\n",
              " 0.031633249891608456,\n",
              " 0.0456528294365567,\n",
              " 0.025888083266432507,\n",
              " 0.07644431980065283,\n",
              " 0.05451347532436734,\n",
              " 0.011288648612364054,\n",
              " 0.020185110753489166,\n",
              " 0.024887461490161904,\n",
              " 0.0231213136449041]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ok68haZ7M-m"
      },
      "source": [
        "fdata.to_csv('/content/drive/My Drive/fyp/fdata.csv')   #forcasted dataset saved in .csv format"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}