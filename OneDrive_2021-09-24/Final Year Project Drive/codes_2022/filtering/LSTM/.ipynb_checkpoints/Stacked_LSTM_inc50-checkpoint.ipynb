{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import optuna\n",
    "from numpy import array\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#code for MAPE, referred from the url: https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error\n",
    "#'eps' is an arbitrary small yet strictly positive number to avoid undefined results when y is zero.\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    eps=0.01\n",
    "    for i in range(len(y_true)):\n",
    "      if y_true[i]==0.00:\n",
    "        y_true[i]=eps\n",
    "    return np.mean((np.abs(y_true - y_pred)) / np.abs(y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIANPAINT</th>\n",
       "      <th>BAJFINANCE</th>\n",
       "      <th>BAJAJFINSV</th>\n",
       "      <th>BRITANNIA</th>\n",
       "      <th>DIVISLAB</th>\n",
       "      <th>HCLTECH</th>\n",
       "      <th>HDFCBANK</th>\n",
       "      <th>HINDALCO</th>\n",
       "      <th>HINDUNILVR</th>\n",
       "      <th>HDFC</th>\n",
       "      <th>...</th>\n",
       "      <th>JSWSTEEL</th>\n",
       "      <th>KOTAKBANK</th>\n",
       "      <th>NESTLEIND</th>\n",
       "      <th>RELIANCE</th>\n",
       "      <th>SHREECEM</th>\n",
       "      <th>TCS</th>\n",
       "      <th>TATASTEEL</th>\n",
       "      <th>TECHM</th>\n",
       "      <th>TITAN</th>\n",
       "      <th>WIPRO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850.608887</td>\n",
       "      <td>602.348328</td>\n",
       "      <td>1978.543457</td>\n",
       "      <td>1374.428223</td>\n",
       "      <td>1108.503052</td>\n",
       "      <td>385.106506</td>\n",
       "      <td>528.506653</td>\n",
       "      <td>82.550529</td>\n",
       "      <td>792.340698</td>\n",
       "      <td>1169.114624</td>\n",
       "      <td>...</td>\n",
       "      <td>93.927940</td>\n",
       "      <td>725.564270</td>\n",
       "      <td>5311.719238</td>\n",
       "      <td>486.556549</td>\n",
       "      <td>11121.029297</td>\n",
       "      <td>1082.013306</td>\n",
       "      <td>219.908173</td>\n",
       "      <td>461.389832</td>\n",
       "      <td>340.842285</td>\n",
       "      <td>202.975677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>852.593201</td>\n",
       "      <td>598.059875</td>\n",
       "      <td>1954.307739</td>\n",
       "      <td>1368.077148</td>\n",
       "      <td>1096.297607</td>\n",
       "      <td>385.152100</td>\n",
       "      <td>519.647644</td>\n",
       "      <td>78.515373</td>\n",
       "      <td>794.607056</td>\n",
       "      <td>1130.328369</td>\n",
       "      <td>...</td>\n",
       "      <td>95.546753</td>\n",
       "      <td>703.665161</td>\n",
       "      <td>5205.810547</td>\n",
       "      <td>476.948639</td>\n",
       "      <td>10804.255859</td>\n",
       "      <td>1061.057739</td>\n",
       "      <td>219.481003</td>\n",
       "      <td>458.151581</td>\n",
       "      <td>340.212952</td>\n",
       "      <td>203.431641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>871.807617</td>\n",
       "      <td>592.664673</td>\n",
       "      <td>1940.843384</td>\n",
       "      <td>1365.361816</td>\n",
       "      <td>1094.199951</td>\n",
       "      <td>383.717926</td>\n",
       "      <td>515.715759</td>\n",
       "      <td>80.314178</td>\n",
       "      <td>784.385376</td>\n",
       "      <td>1123.546631</td>\n",
       "      <td>...</td>\n",
       "      <td>99.058762</td>\n",
       "      <td>706.109497</td>\n",
       "      <td>5199.016113</td>\n",
       "      <td>481.668732</td>\n",
       "      <td>10867.756836</td>\n",
       "      <td>1051.810791</td>\n",
       "      <td>234.346603</td>\n",
       "      <td>463.031219</td>\n",
       "      <td>336.630768</td>\n",
       "      <td>203.139801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>858.013977</td>\n",
       "      <td>603.474854</td>\n",
       "      <td>1933.413330</td>\n",
       "      <td>1381.469727</td>\n",
       "      <td>1084.378296</td>\n",
       "      <td>383.080475</td>\n",
       "      <td>517.997192</td>\n",
       "      <td>78.320908</td>\n",
       "      <td>779.852783</td>\n",
       "      <td>1123.453613</td>\n",
       "      <td>...</td>\n",
       "      <td>96.781441</td>\n",
       "      <td>701.969116</td>\n",
       "      <td>5251.443359</td>\n",
       "      <td>494.631134</td>\n",
       "      <td>11188.242188</td>\n",
       "      <td>1066.431152</td>\n",
       "      <td>229.604980</td>\n",
       "      <td>465.870178</td>\n",
       "      <td>336.776001</td>\n",
       "      <td>202.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>844.413879</td>\n",
       "      <td>603.380981</td>\n",
       "      <td>1921.943726</td>\n",
       "      <td>1356.088257</td>\n",
       "      <td>1074.604370</td>\n",
       "      <td>375.636505</td>\n",
       "      <td>512.706055</td>\n",
       "      <td>74.528839</td>\n",
       "      <td>758.761841</td>\n",
       "      <td>1095.722656</td>\n",
       "      <td>...</td>\n",
       "      <td>93.868484</td>\n",
       "      <td>690.046875</td>\n",
       "      <td>5177.484863</td>\n",
       "      <td>485.550232</td>\n",
       "      <td>10725.758789</td>\n",
       "      <td>1061.796509</td>\n",
       "      <td>213.500595</td>\n",
       "      <td>453.183197</td>\n",
       "      <td>333.096954</td>\n",
       "      <td>200.568176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>2557.899902</td>\n",
       "      <td>4736.549805</td>\n",
       "      <td>10001.750000</td>\n",
       "      <td>3526.107910</td>\n",
       "      <td>3784.350098</td>\n",
       "      <td>918.745850</td>\n",
       "      <td>1404.800049</td>\n",
       "      <td>348.350006</td>\n",
       "      <td>2360.649902</td>\n",
       "      <td>2509.800049</td>\n",
       "      <td>...</td>\n",
       "      <td>655.799988</td>\n",
       "      <td>1759.650024</td>\n",
       "      <td>16779.318359</td>\n",
       "      <td>1937.849976</td>\n",
       "      <td>28062.599609</td>\n",
       "      <td>3085.706299</td>\n",
       "      <td>940.750000</td>\n",
       "      <td>962.200012</td>\n",
       "      <td>1479.849976</td>\n",
       "      <td>480.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>2574.350098</td>\n",
       "      <td>4865.049805</td>\n",
       "      <td>10091.349609</td>\n",
       "      <td>3528.100342</td>\n",
       "      <td>3908.949951</td>\n",
       "      <td>918.795288</td>\n",
       "      <td>1438.699951</td>\n",
       "      <td>366.250000</td>\n",
       "      <td>2379.850098</td>\n",
       "      <td>2518.399902</td>\n",
       "      <td>...</td>\n",
       "      <td>665.900024</td>\n",
       "      <td>1750.300049</td>\n",
       "      <td>16688.214844</td>\n",
       "      <td>1988.650024</td>\n",
       "      <td>28098.550781</td>\n",
       "      <td>3116.754150</td>\n",
       "      <td>977.750000</td>\n",
       "      <td>969.250000</td>\n",
       "      <td>1495.099976</td>\n",
       "      <td>485.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>2614.550049</td>\n",
       "      <td>5280.899902</td>\n",
       "      <td>10489.299805</td>\n",
       "      <td>3465.881348</td>\n",
       "      <td>3882.600098</td>\n",
       "      <td>913.799988</td>\n",
       "      <td>1476.800049</td>\n",
       "      <td>362.600006</td>\n",
       "      <td>2406.550049</td>\n",
       "      <td>2577.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>662.650024</td>\n",
       "      <td>1811.449951</td>\n",
       "      <td>16543.800781</td>\n",
       "      <td>1997.300049</td>\n",
       "      <td>28687.550781</td>\n",
       "      <td>3108.892822</td>\n",
       "      <td>971.400024</td>\n",
       "      <td>977.400024</td>\n",
       "      <td>1508.849976</td>\n",
       "      <td>489.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>2613.449951</td>\n",
       "      <td>5484.850098</td>\n",
       "      <td>11176.549805</td>\n",
       "      <td>3456.067871</td>\n",
       "      <td>3910.850098</td>\n",
       "      <td>909.549988</td>\n",
       "      <td>1472.500000</td>\n",
       "      <td>372.149994</td>\n",
       "      <td>2407.600098</td>\n",
       "      <td>2538.850098</td>\n",
       "      <td>...</td>\n",
       "      <td>726.500000</td>\n",
       "      <td>1805.000000</td>\n",
       "      <td>16502.550781</td>\n",
       "      <td>2024.050049</td>\n",
       "      <td>28444.349609</td>\n",
       "      <td>3100.085693</td>\n",
       "      <td>1031.349976</td>\n",
       "      <td>976.900024</td>\n",
       "      <td>1506.800049</td>\n",
       "      <td>489.850006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>2536.399902</td>\n",
       "      <td>5451.899902</td>\n",
       "      <td>11041.650391</td>\n",
       "      <td>3436.241455</td>\n",
       "      <td>4062.350098</td>\n",
       "      <td>898.950012</td>\n",
       "      <td>1412.300049</td>\n",
       "      <td>364.399994</td>\n",
       "      <td>2353.750000</td>\n",
       "      <td>2420.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>717.849976</td>\n",
       "      <td>1748.800049</td>\n",
       "      <td>16309.250000</td>\n",
       "      <td>1994.500000</td>\n",
       "      <td>27910.500000</td>\n",
       "      <td>3020.873291</td>\n",
       "      <td>1034.000000</td>\n",
       "      <td>960.400024</td>\n",
       "      <td>1491.650024</td>\n",
       "      <td>492.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1314 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASIANPAINT   BAJFINANCE    BAJAJFINSV    BRITANNIA     DIVISLAB  \\\n",
       "0      850.608887   602.348328   1978.543457  1374.428223  1108.503052   \n",
       "1      852.593201   598.059875   1954.307739  1368.077148  1096.297607   \n",
       "2      871.807617   592.664673   1940.843384  1365.361816  1094.199951   \n",
       "3      858.013977   603.474854   1933.413330  1381.469727  1084.378296   \n",
       "4      844.413879   603.380981   1921.943726  1356.088257  1074.604370   \n",
       "...           ...          ...           ...          ...          ...   \n",
       "1309  2557.899902  4736.549805  10001.750000  3526.107910  3784.350098   \n",
       "1310  2574.350098  4865.049805  10091.349609  3528.100342  3908.949951   \n",
       "1311  2614.550049  5280.899902  10489.299805  3465.881348  3882.600098   \n",
       "1312  2613.449951  5484.850098  11176.549805  3456.067871  3910.850098   \n",
       "1313  2536.399902  5451.899902  11041.650391  3436.241455  4062.350098   \n",
       "\n",
       "         HCLTECH     HDFCBANK    HINDALCO   HINDUNILVR         HDFC  ...  \\\n",
       "0     385.106506   528.506653   82.550529   792.340698  1169.114624  ...   \n",
       "1     385.152100   519.647644   78.515373   794.607056  1130.328369  ...   \n",
       "2     383.717926   515.715759   80.314178   784.385376  1123.546631  ...   \n",
       "3     383.080475   517.997192   78.320908   779.852783  1123.453613  ...   \n",
       "4     375.636505   512.706055   74.528839   758.761841  1095.722656  ...   \n",
       "...          ...          ...         ...          ...          ...  ...   \n",
       "1309  918.745850  1404.800049  348.350006  2360.649902  2509.800049  ...   \n",
       "1310  918.795288  1438.699951  366.250000  2379.850098  2518.399902  ...   \n",
       "1311  913.799988  1476.800049  362.600006  2406.550049  2577.000000  ...   \n",
       "1312  909.549988  1472.500000  372.149994  2407.600098  2538.850098  ...   \n",
       "1313  898.950012  1412.300049  364.399994  2353.750000  2420.100098  ...   \n",
       "\n",
       "        JSWSTEEL    KOTAKBANK     NESTLEIND     RELIANCE      SHREECEM  \\\n",
       "0      93.927940   725.564270   5311.719238   486.556549  11121.029297   \n",
       "1      95.546753   703.665161   5205.810547   476.948639  10804.255859   \n",
       "2      99.058762   706.109497   5199.016113   481.668732  10867.756836   \n",
       "3      96.781441   701.969116   5251.443359   494.631134  11188.242188   \n",
       "4      93.868484   690.046875   5177.484863   485.550232  10725.758789   \n",
       "...          ...          ...           ...          ...           ...   \n",
       "1309  655.799988  1759.650024  16779.318359  1937.849976  28062.599609   \n",
       "1310  665.900024  1750.300049  16688.214844  1988.650024  28098.550781   \n",
       "1311  662.650024  1811.449951  16543.800781  1997.300049  28687.550781   \n",
       "1312  726.500000  1805.000000  16502.550781  2024.050049  28444.349609   \n",
       "1313  717.849976  1748.800049  16309.250000  1994.500000  27910.500000   \n",
       "\n",
       "              TCS    TATASTEEL       TECHM        TITAN       WIPRO  \n",
       "0     1082.013306   219.908173  461.389832   340.842285  202.975677  \n",
       "1     1061.057739   219.481003  458.151581   340.212952  203.431641  \n",
       "2     1051.810791   234.346603  463.031219   336.630768  203.139801  \n",
       "3     1066.431152   229.604980  465.870178   336.776001  202.483200  \n",
       "4     1061.796509   213.500595  453.183197   333.096954  200.568176  \n",
       "...           ...          ...         ...          ...         ...  \n",
       "1309  3085.706299   940.750000  962.200012  1479.849976  480.299988  \n",
       "1310  3116.754150   977.750000  969.250000  1495.099976  485.049988  \n",
       "1311  3108.892822   971.400024  977.400024  1508.849976  489.299988  \n",
       "1312  3100.085693  1031.349976  976.900024  1506.800049  489.850006  \n",
       "1313  3020.873291  1034.000000  960.400024  1491.650024  492.750000  \n",
       "\n",
       "[1314 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=pd.read_csv('n50.csv',parse_dates=['Date'],index_col='Date')\n",
    "x = x.loc[\"2016-01-01\" :]                         #Since 2016-01-01, 5y(1234rows till 2020-12-31), + year 2021's rows (till 30th of April)\n",
    "y=x.copy()                                        #deep copy\n",
    "x.reset_index(drop=True, inplace=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stonks=[]\n",
    "for i in x:\n",
    "  stonks.append(i)\n",
    "len(stonks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alldata=x   #the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#timesteps=60                                     #lstm hyperparameters \"Subject to be tuned\"\n",
    "#epoch=variable\n",
    "#batchSize=32\n",
    "#ineurons=175\n",
    "#hneurons=187\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, time_step=1):         # convert an array of values into a dataset matrix which will be used to train the lstm model.\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-time_step):\n",
    "\t\ta = dataset[i:(i+time_step), 0]               #i=0, 0,1,2,3-----(timesteps-1)  -> timesteps\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + time_step, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static parameters\n",
    "after2020=len(y.loc[\"2021-01-01\" : ])                    #number of days after 31-12-2020 \"automated\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_lstm_model(df1,batchSize,dropout_value,after2020):\n",
    "  timesteps=60\n",
    "  scaler=MinMaxScaler(feature_range=(0,1))\n",
    "  df1=scaler.fit_transform(np.array(df1).reshape(-1,1))           #minmax scalar transformation of data\n",
    "  before_2021_data_length=int(len(df1)-after2020)                 #length of data before 2021\n",
    "  training_size=int(before_2021_data_length*0.80)                 #80% of training size, refered from Yadav et al (2020) (Science Direct)\n",
    "  train_data=df1[0:training_size,:]\n",
    "  test_data=df1[training_size:before_2021_data_length,:1]         #20% of testing data, refered from Yadav et al (2020) (Science Direct)\n",
    "  inpdata=df1[before_2021_data_length-timesteps:len(df1),:1]      #getting the data from 01-01-2021 onwards\n",
    "\n",
    "\n",
    "  #reshape into X=t,t+1,t+2,t+3,........t+\"timestep-1\" and Y=t+\"timestep\"\n",
    "  X_train, y_train = create_dataset(train_data, timesteps)\n",
    "  x_inp, y_inp = create_dataset(inpdata, timesteps)\n",
    "  x_test, y_test = create_dataset(test_data,timesteps)\n",
    "\n",
    "  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "  x_inp = x_inp.reshape(x_inp.shape[0],x_inp.shape[1] , 1)        #reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "  x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)\n",
    "\n",
    "  # initialising stacked lstm\n",
    "  model=Sequential()\n",
    "  model.add(LSTM(60,return_sequences=True,input_shape=(timesteps,1),activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(264,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(80,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(416,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(160,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(472,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(440,activation='tanh', dropout=0.1))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "\n",
    "  model.fit(X_train,y_train,validation_data=(x_test,y_test),epochs=10,batch_size=batchSize,verbose=1)     # training of the model\n",
    "\n",
    "  test_predict=model.predict(x_test)                    #prediction using test data as input\n",
    "\n",
    "  #performance metrics between, original test data and predicted test data\n",
    "  msetst =mean_squared_error(y_test,test_predict)\n",
    "  rmsetst=math.sqrt(msetst)\n",
    "  maetst =mean_absolute_error(y_test,test_predict)\n",
    "  r2tst  =r2_score(y_test,test_predict)\n",
    "  mapetst=mean_absolute_percentage_error(y_test,test_predict)\n",
    "  tstlst =[msetst,rmsetst,maetst,r2tst,mapetst]\n",
    "\n",
    "\n",
    "  #model is trained again on the test data so as to increase the learning (it is often termed as incremental learning)\n",
    "  #refered from url: https://www.justintodata.com/forecast-time-series-lstm-with-tensorflow-keras/#step-2-transforming-the-dataset-for-tensorflow-keras\n",
    "  #refered from url: https://github.com/keras-team/keras/issues/4446\n",
    "  model.fit(x_test,y_test,epochs=10,batch_size=batchSize,verbose=1)\n",
    "\n",
    "  out_predict=model.predict(x_inp)                      #dynamic prediction of the stock's closing price from 01-01-2021 onwards\n",
    "\n",
    "  #performance metrics between, original data(after 31-12-2020) and dynamically predicted data (after 31-12-2020)\n",
    "  mseinp =mean_squared_error(y_inp,out_predict)\n",
    "  rmseinp=math.sqrt(mseinp)\n",
    "  maeinp =mean_absolute_error(y_inp,out_predict)\n",
    "  r2inp  =r2_score(y_inp,out_predict)\n",
    "  mapeinp=mean_absolute_percentage_error(y_inp,out_predict)\n",
    "  inplst =[mseinp,rmseinp,maeinp,r2inp,mapeinp]\n",
    "\n",
    "\n",
    "  lst=[]\n",
    "  for i in out_predict:\n",
    "    lst.append(i)\n",
    "\n",
    "  p=train_data.tolist()\n",
    "  q=test_data.tolist()\n",
    "  p.extend(q)                                         #appending train and test data to make dataset before 2021 (data till 31-12-2020)\n",
    "  p.extend(lst)                                       #appending the data, forcasted from 01-01-2021 onwards, to the data till 31-12-2020\n",
    "  p=scaler.inverse_transform(p).tolist()\n",
    "\n",
    "  return rmseinp\n",
    "  #returns a dataframe, tstlst => test performance metrics, inplst => forcasted data performance metrics\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_lstm_tuning(batchSize,dropout_value, after2020):\n",
    "    for i in alldata:                                   # this for loop will be iterated for each column of the original dataset\n",
    "      df1=alldata[i]\n",
    "      rmseinp=stacked_lstm_model(df1, batchSize,dropout_value, after2020)    #hyperparameters are provided as input here\n",
    "    return rmseinp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    batchSize=trial.suggest_int('batchSize',8,128,step=8)\n",
    "    dropout_value=trial.suggest_float('dropout_value',0.1,0.5)\n",
    "    return stacked_lstm_tuning(int(batchSize), dropout_value, after2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-28 14:21:14,238]\u001b[0m A new study created in memory with name: no-name-e2818e71-fe5f-4b36-9a8c-813b4aec784e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 35s 3s/step - loss: 0.0603 - val_loss: 0.3199\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0309 - val_loss: 0.1631\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0151 - val_loss: 0.1653\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0132 - val_loss: 0.1525\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0127 - val_loss: 0.1412\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0128 - val_loss: 0.1208\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 32s 4s/step - loss: 0.0132 - val_loss: 0.1319\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0130 - val_loss: 0.1398\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0127 - val_loss: 0.1464\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0128 - val_loss: 0.1514\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.1277\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0281\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0590\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: 0.0635\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0413\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0243\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0276\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.0309\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0270\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.0232\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 45s 4s/step - loss: 0.0549 - val_loss: 0.0492\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0268 - val_loss: 0.0458\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0084 - val_loss: 0.0320\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0040 - val_loss: 0.0321\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0034 - val_loss: 0.0234\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0022 - val_loss: 0.0224\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0019 - val_loss: 0.0214\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0019 - val_loss: 0.0207\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 33s 4s/step - loss: 0.0017 - val_loss: 0.0196\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0018 - val_loss: 0.0190\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0197\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0180\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0165\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0161\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0118\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0110\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0101\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0133\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0084\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0085\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 48s 4s/step - loss: 0.0490 - val_loss: 0.0239\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0231 - val_loss: 0.0223\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0089 - val_loss: 0.0217\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 31s 4s/step - loss: 0.0051 - val_loss: 0.0229\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0044 - val_loss: 0.0218\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 32s 4s/step - loss: 0.0035 - val_loss: 0.0200\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 36s 4s/step - loss: 0.0034 - val_loss: 0.0197\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.0034 - val_loss: 0.0177\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.0035 - val_loss: 0.0158\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 36s 4s/step - loss: 0.0032 - val_loss: 0.0150\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0153\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0140\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0131\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0117\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0107\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0102\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0097\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0087\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0077\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0087\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 53s 5s/step - loss: 0.0499 - val_loss: 0.1276\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0166 - val_loss: 0.0552\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0072 - val_loss: 0.0818\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0048 - val_loss: 0.0369\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0039 - val_loss: 0.0435\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0035 - val_loss: 0.0464\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0034 - val_loss: 0.0315\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0034 - val_loss: 0.0377\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 65s 7s/step - loss: 0.0033 - val_loss: 0.0358\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0032 - val_loss: 0.0404\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0298\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0146\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0217\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0147\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0155\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0142\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0132\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0135\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0130\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0121\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 36s 3s/step - loss: 0.0574 - val_loss: 0.4629\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0431 - val_loss: 0.4628\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0428 - val_loss: 0.3904\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0127 - val_loss: 0.2637\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 33s 4s/step - loss: 0.0101 - val_loss: 0.2517\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0104 - val_loss: 0.2780\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0111 - val_loss: 0.2438\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0102 - val_loss: 0.2083\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0114 - val_loss: 0.2321\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0108 - val_loss: 0.2413\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.1987\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0309\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0558\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0715\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0715\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0640\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0517\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.0375\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0264\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.0231\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 57s 5s/step - loss: 0.0541 - val_loss: 0.2850\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 31s 4s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 31s 4s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 32s 4s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.0339 - val_loss: 0.2850\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 32s 4s/step - loss: 0.0339 - val_loss: 0.2849\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.2849\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.2831\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.1743\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.1129\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0930\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0488\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0677\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 8s 5s/step - loss: 0.0417\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0464\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0493\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 37s 3s/step - loss: 0.0437 - val_loss: 0.0257\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0254 - val_loss: 0.0440\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0096 - val_loss: 0.0160\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0034 - val_loss: 0.0201\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0029 - val_loss: 0.0115\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0021 - val_loss: 0.0097\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0020 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0018 - val_loss: 0.0110\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0018 - val_loss: 0.0114\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0018 - val_loss: 0.0102\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0102\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0090\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0089\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0090\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0081\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0069\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0064\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0071\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0063\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0064\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.0191 - val_loss: 0.0170\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 45s 5s/step - loss: 0.0162 - val_loss: 0.0219\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 0.0036 - val_loss: 0.0095\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 45s 5s/step - loss: 0.0029 - val_loss: 0.0077\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 45s 5s/step - loss: 0.0026 - val_loss: 0.0071\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 0.0024 - val_loss: 0.0068\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 0.0024 - val_loss: 0.0063\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 0.0026 - val_loss: 0.0062\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 0.0025 - val_loss: 0.0081\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0076\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.0080\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0057\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0058\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0051\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0048\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0045\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0038\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0038\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 7s 3s/step - loss: 0.0043\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 53s 5s/step - loss: 0.0553 - val_loss: 0.1701\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 36s 4s/step - loss: 0.0212 - val_loss: 0.0597\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 35s 4s/step - loss: 0.0074 - val_loss: 0.0580\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 35s 4s/step - loss: 0.0041 - val_loss: 0.0237\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 36s 4s/step - loss: 0.0032 - val_loss: 0.0080\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1734s 216s/step - loss: 0.0027 - val_loss: 0.0291\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.0025 - val_loss: 0.0208\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 35s 4s/step - loss: 0.0023 - val_loss: 0.0129\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 36s 4s/step - loss: 0.0022 - val_loss: 0.0151\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 38s 4s/step - loss: 0.0022 - val_loss: 0.0178\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 7s 4s/step - loss: 0.0142\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0118\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0065\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0063\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0054\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0056\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0040\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 8s 5s/step - loss: 0.0050\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0045\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0041\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 49s 5s/step - loss: 0.0369 - val_loss: 0.0279\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.0248 - val_loss: 0.0295\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 38s 4s/step - loss: 0.0179 - val_loss: 0.0162\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 40s 5s/step - loss: 0.0064 - val_loss: 0.0156\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 34s 4s/step - loss: 0.0049 - val_loss: 0.0143\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 46s 5s/step - loss: 0.0033 - val_loss: 0.0126\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 36s 4s/step - loss: 0.0029 - val_loss: 0.0110\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 39s 4s/step - loss: 0.0027 - val_loss: 0.0102\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 35s 4s/step - loss: 0.0027 - val_loss: 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "9/9 [==============================] - 42s 5s/step - loss: 0.0025 - val_loss: 0.0089\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.0101\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0096\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0086\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 11s 5s/step - loss: 0.0082\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0076\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0065\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0063\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0076\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0068\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0062\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 61s 6s/step - loss: 0.0454 - val_loss: 0.0246\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 46s 5s/step - loss: 0.0238 - val_loss: 0.0229\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 47s 5s/step - loss: 0.0193 - val_loss: 0.0249\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 48s 5s/step - loss: 0.0096 - val_loss: 0.0175\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 0.0057 - val_loss: 0.0103\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 48s 5s/step - loss: 0.0036 - val_loss: 0.0109\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 48s 6s/step - loss: 0.0024 - val_loss: 0.0104\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 48s 6s/step - loss: 0.0023 - val_loss: 0.0094\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 48s 5s/step - loss: 0.0023 - val_loss: 0.0093\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 0.0023 - val_loss: 0.0092\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0096\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.0081\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0080\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0080\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0074\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0064\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0065\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0054\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0053\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0060\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 77s 7s/step - loss: 0.0553 - val_loss: 0.2782\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 59s 7s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 53s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0354 - val_loss: 0.2782\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.2782\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.2782\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 52s 5s/step - loss: 0.0344 - val_loss: 0.0265\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 39s 4s/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 39s 4s/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 40s 5s/step - loss: 0.0019 - val_loss: 0.0063\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 40s 5s/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 39s 4s/step - loss: 0.0012 - val_loss: 0.0032\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 40s 5s/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 38s 4s/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0030\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0029\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0023\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0022\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0020\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0020\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0019\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0016\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0017\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 8s 4s/step - loss: 0.0016\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 74s 7s/step - loss: 0.0475 - val_loss: 0.0366\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0104 - val_loss: 0.0310\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0056 - val_loss: 0.0309\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 783s 97s/step - loss: 0.0039 - val_loss: 0.0262\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 0.0030 - val_loss: 0.0200\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0024 - val_loss: 0.0190\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 56s 6s/step - loss: 0.0025 - val_loss: 0.0183\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 58s 7s/step - loss: 0.0024 - val_loss: 0.0175\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 65s 7s/step - loss: 0.0023 - val_loss: 0.0169\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 66s 7s/step - loss: 0.0023 - val_loss: 0.0161\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 12s 5s/step - loss: 0.0177\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 11s 5s/step - loss: 0.0161\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 11s 5s/step - loss: 0.0135\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0112\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 12s 5s/step - loss: 0.0138\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 12s 5s/step - loss: 0.0111\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 11s 5s/step - loss: 0.0095\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 12s 5s/step - loss: 0.0105\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0096\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0080\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 63s 6s/step - loss: 0.0544 - val_loss: 0.0111\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 51s 6s/step - loss: 0.0125 - val_loss: 0.0064\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0058 - val_loss: 0.0070\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 50s 5s/step - loss: 0.0035 - val_loss: 0.0152\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 0.0023 - val_loss: 0.0137\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 55s 6s/step - loss: 0.0020 - val_loss: 0.0204\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 55s 6s/step - loss: 0.0018 - val_loss: 0.0324\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 52s 6s/step - loss: 0.0019 - val_loss: 0.0134\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 0.0021 - val_loss: 0.0138\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 51s 6s/step - loss: 0.0018 - val_loss: 0.0113\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0143\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0200\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0094\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0095\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0038\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 11s 4s/step - loss: 0.0045\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0043\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0032\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0038\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.0032\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 52s 5s/step - loss: 0.0611 - val_loss: 0.2988\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 40s 4s/step - loss: 0.0288 - val_loss: 0.2945\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 40s 4s/step - loss: 0.0199 - val_loss: 0.0551\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 41s 5s/step - loss: 0.0148 - val_loss: 0.0088\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 40s 4s/step - loss: 0.0060 - val_loss: 0.0600\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 40s 4s/step - loss: 0.0028 - val_loss: 0.0551\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 40s 4s/step - loss: 0.0020 - val_loss: 0.0520\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 41s 4s/step - loss: 0.0017 - val_loss: 0.0490\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 40s 4s/step - loss: 0.0016 - val_loss: 0.0362\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 38s 4s/step - loss: 0.0015 - val_loss: 0.0313\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0274\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0247\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0166\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.0128\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0066\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.0090\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 11s 5s/step - loss: 0.0089\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0062\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.0078\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0061\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 81s 8s/step - loss: 0.0228 - val_loss: 0.1047\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 70s 8s/step - loss: 0.0120 - val_loss: 0.0453\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 70s 8s/step - loss: 0.0088 - val_loss: 0.0375\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 69s 8s/step - loss: 0.0082 - val_loss: 0.0443\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 69s 8s/step - loss: 0.0072 - val_loss: 0.0150\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 68s 8s/step - loss: 0.0047 - val_loss: 0.0097\n",
      "Epoch 7/10\n",
      "4/9 [============>.................] - ETA: 41s - loss: 0.0037"
     ]
    }
   ],
   "source": [
    "lstm_parameter_selection=optuna.create_study(direction='minimize')\n",
    "lstm_parameter_selection.optimize(objective,n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=lstm_parameter_selection.best_params\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize=best['batchSize']\n",
    "dropout=best['dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stacked_lstm_forecast(df1, batchSize, after2020):\n",
    "  timesteps=60\n",
    "  scaler=MinMaxScaler(feature_range=(0,1))\n",
    "  df1=scaler.fit_transform(np.array(df1).reshape(-1,1))           #minmax scalar transformation of data\n",
    "\n",
    "  before_2021_data_length=int(len(df1)-after2020)                 #length of data before 2021\n",
    "  training_size=int(before_2021_data_length*0.80)                 #80% of training size, refered from Yadav et al (2020) (Science Direct)\n",
    "  train_data=df1[0:training_size,:]\n",
    "  test_data=df1[training_size:before_2021_data_length,:1]         #20% of testing data, refered from Yadav et al (2020) (Science Direct)\n",
    "  inpdata=df1[before_2021_data_length-timesteps:len(df1),:1]      #getting the data from 01-01-2021 onwards\n",
    "\n",
    "\n",
    "  #reshape into X=t,t+1,t+2,t+3,........t+\"timestep-1\" and Y=t+\"timestep\"\n",
    "  X_train, y_train = create_dataset(train_data, timesteps)\n",
    "  x_inp, y_inp = create_dataset(inpdata, timesteps)\n",
    "  x_test, y_test = create_dataset(test_data,timesteps)\n",
    "\n",
    "  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "  x_inp = x_inp.reshape(x_inp.shape[0],x_inp.shape[1] , 1)        #reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "  x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)\n",
    "\n",
    "  # initialising stacked lstm\n",
    "  model=Sequential()\n",
    "  model.add(LSTM(60,return_sequences=True,input_shape=(timesteps,1),activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(264,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(80,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(416,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(160,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(472,return_sequences=True,activation='tanh', dropout=0.1))\n",
    "  model.add(LSTM(440,activation='tanh', dropout=0.1))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "\n",
    "  model.fit(X_train,y_train,validation_data=(x_test,y_test),epochs=10,batch_size=batchSize,verbose=1)     # training of the model\n",
    "\n",
    "  test_predict=model.predict(x_test)                    #prediction using test data as input\n",
    "\n",
    "  #performance metrics between, original test data and predicted test data\n",
    "  msetst =mean_squared_error(y_test,test_predict)\n",
    "  rmsetst=math.sqrt(msetst)\n",
    "  maetst =mean_absolute_error(y_test,test_predict)\n",
    "  r2tst  =r2_score(y_test,test_predict)\n",
    "  mapetst=mean_absolute_percentage_error(y_test,test_predict)\n",
    "  tstlst =[msetst,rmsetst,maetst,r2tst,mapetst]\n",
    "\n",
    "\n",
    "  #model is trained again on the test data so as to increase the learning (it is often termed as incremental learning)\n",
    "  #refered from url: https://www.justintodata.com/forecast-time-series-lstm-with-tensorflow-keras/#step-2-transforming-the-dataset-for-tensorflow-keras\n",
    "  #refered from url: https://github.com/keras-team/keras/issues/4446\n",
    "  model.fit(x_test,y_test,epochs=10,batch_size=batchSize,verbose=1)\n",
    "\n",
    "  out_predict=model.predict(x_inp)                      #dynamic prediction of the stock's closing price from 01-01-2021 onwards\n",
    "\n",
    "  #performance metrics between, original data(after 31-12-2020) and dynamically predicted data (after 31-12-2020)\n",
    "  mseinp =mean_squared_error(y_inp,out_predict)\n",
    "  rmseinp=math.sqrt(mseinp)\n",
    "  maeinp =mean_absolute_error(y_inp,out_predict)\n",
    "  r2inp  =r2_score(y_inp,out_predict)\n",
    "  mapeinp=mean_absolute_percentage_error(y_inp,out_predict)\n",
    "  inplst =[mseinp,rmseinp,maeinp,r2inp,mapeinp]\n",
    "\n",
    "\n",
    "  lst=[]\n",
    "  for i in out_predict:\n",
    "    lst.append(i)\n",
    "\n",
    "  p=train_data.tolist()\n",
    "  q=test_data.tolist()\n",
    "  p.extend(q)                                         #appending train and test data to make dataset before 2021 (data till 31-12-2020)\n",
    "  p.extend(lst)                                       #appending the data, forcasted from 01-01-2021 onwards, to the data till 31-12-2020\n",
    "  p=scaler.inverse_transform(p).tolist()\n",
    "\n",
    "  return pd.DataFrame(p), tstlst, inplst\n",
    "  #returns a dataframe, tstlst => test performance metrics, inplst => forcasted data performance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mtest=[]\n",
    "mdynamic=[]\n",
    "fdata=pd.DataFrame()\n",
    "for i in alldata:                                   # this for loop will be iterated for 42 times i.e. for each column of the original dataset\n",
    "  temp=alldata[i]\n",
    "  ftemp,trmse,drmse=stacked_lstm_forecast(temp, batchSize, after2020)    #hyperparameters are provided as input here\n",
    "  fdata = pd.concat([fdata,ftemp],axis = 1)\n",
    "  mtest.append(trmse)\n",
    "  mdynamic.append(drmse)\n",
    "fdata.columns=stonks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fdata # dataset with 2021 rows forcasted dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alldata # dataset with original 2021 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fdata.to_csv('data_inc/fdata.csv')   #dataset saved in .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clm=['MSE','RMSE','MAE','R2','MAPE']\n",
    "pd.DataFrame(mtest,index=stonks,columns=clm).to_csv('data_inc/mtest.csv') #metric values saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mdynamic,index=stonks,columns=clm).to_csv('data_inc/mdynamic.csv') #metric values saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
